{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9062f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "from catboost import CatBoostClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "from imblearn.combine import SMOTETomek\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28173b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv(\"../merge/merged_x.csv\")\n",
    "y = pd.read_csv(\"../merge/merged_y.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80e7d5e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¨ Feature engineering...\n",
      "{'numerical': ['loan_amount', 'interest_rate', 'loan_to_value_ratio', 'age', 'annual_income', 'employment_length', 'regional_unemployment_rate', 'regional_median_income', 'regional_median_rent', 'housing_price_index', 'cost_of_living_index', 'num_credit_accounts', 'oldest_credit_line_age', 'oldest_account_age_months', 'total_credit_limit', 'account_diversity_index', 'application_hour', 'num_login_sessions', 'num_customer_service_calls', 'account_tenure', 'monthly_income', 'existing_monthly_debt', 'monthly_payment', 'debt_service_ratio', 'payment_to_income_ratio', 'credit_utilization', 'credit_usage_amount', 'available_credit', 'total_monthly_debt_payment', 'annual_debt_payment', 'loan_to_annual_income', 'total_debt_amount', 'monthly_free_cash_flow'], 'categorical': ['loan_type', 'loan_term', 'loan_purpose', 'origination_channel', 'employment_type', 'education', 'marital_status', 'num_dependents', 'num_delinquencies_2yrs', 'num_inquiries_6mo', 'recent_inquiry_count', 'num_public_records', 'num_collections', 'application_day_of_week', 'preferred_contact', 'account_status_code', 'has_mobile_app', 'paperless_billing', 'is_referred']}\n",
      "ðŸ” Feature types avtomatik aniqlanmoqda...\n",
      "\n",
      "ðŸ“Š Feature Types:\n",
      "   Numerical:   33 features\n",
      "   Categorical: 19 features\n",
      "\n",
      "   Numerical columns: ['loan_amount', 'interest_rate', 'loan_to_value_ratio', 'age', 'annual_income']...\n",
      "   Categorical columns: ['loan_type', 'loan_term', 'loan_purpose', 'origination_channel', 'employment_type']...\n",
      "\n",
      "ðŸ”¨ 1. Polynomial features (faqat numerical)...\n",
      "   âœ“ 19 polynomial features\n",
      "\n",
      "ðŸ”¨ 2. Interaction features (faqat numerical)...\n",
      "   âœ“ 22 interaction features\n",
      "\n",
      "ðŸ”¨ 3. Ratio features (faqat numerical)...\n",
      "   âœ“ 19 ratio features\n",
      "\n",
      "ðŸ”¨ 4. Categorical aggregations...\n",
      "   âœ“ 27 categorical aggregation features\n",
      "\n",
      "ðŸ”¨ 5. Frequency encoding (categorical)...\n",
      "   âœ“ 5 frequency features\n",
      "\n",
      "ðŸ”¨ 6. Statistical aggregation...\n",
      "   âœ“ 6 statistical features\n",
      "\n",
      "ðŸ”¨ 7. Binning features (numerical)...\n",
      "   âœ“ 5 binning features\n",
      "\n",
      "ðŸ”¨ 8. Missing value indicators...\n",
      "   âœ“ 0 missing indicators\n",
      "\n",
      "ðŸ”§ Filling missing values...\n",
      "   âœ“ Missing values filled\n",
      "\n",
      "======================================================================\n",
      "âœ… FEATURE ENGINEERING YAKUNLANDI!\n",
      "======================================================================\n",
      "   Original features:     52\n",
      "   New features created:  90\n",
      "   Total features:        142\n",
      "   - Numerical features:  142\n",
      "   - Non-numerical:       0\n",
      "======================================================================\n",
      "âœ“ Original features: 52\n",
      "âœ“ Engineered features: 52\n",
      "âœ“ Train: (57599, 52)\n",
      "âœ“ Val:   (14400, 52)\n",
      "âœ“ Test:  (18000, 52)\n",
      "\n",
      "================================================================================\n",
      "STEP 2: FEATURE SELECTION\n",
      "================================================================================\n",
      "ðŸŽ¯ Selecting top 52 features...\n",
      "âœ“ Selected features: 52\n",
      "âœ“ Scaling completed!\n",
      "\n",
      "================================================================================\n",
      "STEP 4: TRAINING ENSEMBLE MODELS\n",
      "================================================================================\n",
      "\n",
      "1. CatBoost (tuned)...\n",
      "   âœ“ Trained\n",
      "\n",
      "2. LightGBM (tuned)...\n",
      "   âœ“ Trained\n",
      "\n",
      "3. XGBoost (tuned)...\n",
      "   âœ“ Trained\n",
      "\n",
      "4. CatBoost + SMOTE...\n",
      "   âœ“ Trained\n",
      "\n",
      "5. CatBoost + ADASYN...\n",
      "   âœ“ Trained\n",
      "\n",
      "6. Calibrated CatBoost...\n",
      "   âœ“ Trained\n",
      "\n",
      "âœ… All 6 models trained successfully!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def identify_feature_types(df, categorical_threshold=10):\n",
    "    \n",
    "    numerical_features = []\n",
    "    categorical_features = []\n",
    "    \n",
    "    for col in df.columns:\n",
    "        # Agar int yoki float bo'lib, unique values ko'p bo'lsa -> numerical\n",
    "        if df[col].dtype in ['int64', 'float64']:\n",
    "            n_unique = df[col].nunique()\n",
    "            if n_unique > categorical_threshold:\n",
    "                numerical_features.append(col)\n",
    "            else:\n",
    "                categorical_features.append(col)\n",
    "        else:\n",
    "            categorical_features.append(col)\n",
    "    \n",
    "    return {\n",
    "        'numerical': numerical_features,\n",
    "        'categorical': categorical_features\n",
    "    }\n",
    "\n",
    "def engineer_features(df, feature_types=None):\n",
    "    df_enh = df.copy()\n",
    "    \n",
    "    # Convert to DataFrame if needed\n",
    "    if not isinstance(df_enh, pd.DataFrame):\n",
    "        df_enh = pd.DataFrame(df_enh, columns=[f'feature_{i}' for i in range(df_enh.shape[1])])\n",
    "    \n",
    "    # Feature types'ni aniqlash\n",
    "    if feature_types is None:\n",
    "        print(\"ðŸ” Feature types avtomatik aniqlanmoqda...\")\n",
    "        feature_types = identify_feature_types(df_enh)\n",
    "    \n",
    "    numerical_cols = feature_types['numerical']\n",
    "    categorical_cols = feature_types['categorical']\n",
    "    \n",
    "    print(f\"\\nðŸ“Š Feature Types:\")\n",
    "    print(f\"   Numerical:   {len(numerical_cols)} features\")\n",
    "    print(f\"   Categorical: {len(categorical_cols)} features\")\n",
    "    print(f\"\\n   Numerical columns: {numerical_cols[:5]}{'...' if len(numerical_cols) > 5 else ''}\")\n",
    "    print(f\"   Categorical columns: {categorical_cols[:5]}{'...' if len(categorical_cols) > 5 else ''}\")\n",
    "    \n",
    "    new_features = []\n",
    "    \n",
    "    print(\"\\nðŸ”¨ 1. Polynomial features (faqat numerical)...\")\n",
    "    \n",
    "    for col in numerical_cols[:min(5, len(numerical_cols))]:\n",
    "        try:\n",
    "            # Squared\n",
    "            df_enh[f'{col}_squared'] = df_enh[col] ** 2\n",
    "            new_features.append(f'{col}_squared')\n",
    "            \n",
    "            # Cubed (agar variability yuqori bo'lsa)\n",
    "            if df_enh[col].std() > 0.1:\n",
    "                df_enh[f'{col}_cubed'] = df_enh[col] ** 3\n",
    "                new_features.append(f'{col}_cubed')\n",
    "            \n",
    "            # Square root (faqat positive values uchun)\n",
    "            if (df_enh[col] >= 0).all():\n",
    "                df_enh[f'{col}_sqrt'] = np.sqrt(df_enh[col])\n",
    "                new_features.append(f'{col}_sqrt')\n",
    "            \n",
    "            # Log transform (faqat positive values uchun)\n",
    "            if (df_enh[col] > 0).all():\n",
    "                df_enh[f'{col}_log'] = np.log1p(df_enh[col])\n",
    "                new_features.append(f'{col}_log')\n",
    "        except Exception as e:\n",
    "            print(f\"   âš ï¸  Xato {col}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"   âœ“ {len([f for f in new_features if any(x in f for x in ['squared', 'cubed', 'sqrt', 'log'])])} polynomial features\")\n",
    "    \n",
    "\n",
    "    print(\"\\nðŸ”¨ 2. Interaction features (faqat numerical)...\")\n",
    "    \n",
    "    if len(numerical_cols) >= 2:\n",
    "        for i in range(min(3, len(numerical_cols))):\n",
    "            for j in range(i+1, min(4, len(numerical_cols))):\n",
    "                col1, col2 = numerical_cols[i], numerical_cols[j]\n",
    "                \n",
    "                try:\n",
    "                    # Multiplication\n",
    "                    df_enh[f'{col1}_x_{col2}'] = df_enh[col1] * df_enh[col2]\n",
    "                    new_features.append(f'{col1}_x_{col2}')\n",
    "                    \n",
    "                    # Division (avoid division by zero)\n",
    "                    if (df_enh[col2] != 0).all() and df_enh[col2].std() > 0:\n",
    "                        df_enh[f'{col1}_div_{col2}'] = df_enh[col1] / (df_enh[col2] + 1e-5)\n",
    "                        new_features.append(f'{col1}_div_{col2}')\n",
    "                    \n",
    "                    # Addition\n",
    "                    df_enh[f'{col1}_plus_{col2}'] = df_enh[col1] + df_enh[col2]\n",
    "                    new_features.append(f'{col1}_plus_{col2}')\n",
    "                    \n",
    "                    # Subtraction (absolute)\n",
    "                    df_enh[f'{col1}_minus_{col2}_abs'] = np.abs(df_enh[col1] - df_enh[col2])\n",
    "                    new_features.append(f'{col1}_minus_{col2}_abs')\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"   âš ï¸  Xato {col1} x {col2}: {str(e)}\")\n",
    "                    continue\n",
    "    \n",
    "    print(f\"   âœ“ {len([f for f in new_features if any(x in f for x in ['_x_', '_div_', '_plus_', '_minus_'])])} interaction features\")\n",
    "    \n",
    "    print(\"\\nðŸ”¨ 3. Ratio features (faqat numerical)...\")\n",
    "    \n",
    "    if len(numerical_cols) >= 2:\n",
    "        for i, col1 in enumerate(numerical_cols[:3]):\n",
    "            for col2 in numerical_cols[i+1:4]:\n",
    "                try:\n",
    "                    if (df_enh[col1] + df_enh[col2]).std() > 0:\n",
    "                        df_enh[f'{col1}_to_{col2}_ratio'] = (\n",
    "                            df_enh[col1] / (df_enh[col1] + df_enh[col2] + 1e-5)\n",
    "                        )\n",
    "                        new_features.append(f'{col1}_to_{col2}_ratio')\n",
    "                except Exception as e:\n",
    "                    continue\n",
    "    \n",
    "    print(f\"   âœ“ {len([f for f in new_features if '_ratio' in f])} ratio features\")\n",
    "    \n",
    "\n",
    "    print(\"\\nðŸ”¨ 4. Categorical aggregations...\")\n",
    "    \n",
    "    # Har bir categorical feature uchun numerical features'ning statistikasi\n",
    "    for cat_col in categorical_cols[:min(3, len(categorical_cols))]:\n",
    "        for num_col in numerical_cols[:min(3, len(numerical_cols))]:\n",
    "            try:\n",
    "                # Mean encoding\n",
    "                mean_encoded = df_enh.groupby(cat_col)[num_col].transform('mean')\n",
    "                df_enh[f'{cat_col}_{num_col}_mean'] = mean_encoded\n",
    "                new_features.append(f'{cat_col}_{num_col}_mean')\n",
    "                \n",
    "                # Std encoding\n",
    "                std_encoded = df_enh.groupby(cat_col)[num_col].transform('std').fillna(0)\n",
    "                df_enh[f'{cat_col}_{num_col}_std'] = std_encoded\n",
    "                new_features.append(f'{cat_col}_{num_col}_std')\n",
    "                \n",
    "                # Count encoding\n",
    "                count_encoded = df_enh.groupby(cat_col)[num_col].transform('count')\n",
    "                df_enh[f'{cat_col}_{num_col}_count'] = count_encoded\n",
    "                new_features.append(f'{cat_col}_{num_col}_count')\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"   âš ï¸  Xato {cat_col} x {num_col}: {str(e)}\")\n",
    "                continue\n",
    "    \n",
    "    print(f\"   âœ“ {len([f for f in new_features if any(x in f for x in ['_mean', '_std', '_count'])])} categorical aggregation features\")\n",
    "    \n",
    "\n",
    "    print(\"\\nðŸ”¨ 5. Frequency encoding (categorical)...\")\n",
    "    \n",
    "    for cat_col in categorical_cols[:min(5, len(categorical_cols))]:\n",
    "        try:\n",
    "            freq = df_enh[cat_col].value_counts(normalize=True)\n",
    "            df_enh[f'{cat_col}_freq'] = df_enh[cat_col].map(freq)\n",
    "            new_features.append(f'{cat_col}_freq')\n",
    "        except Exception as e:\n",
    "            continue\n",
    "    \n",
    "    print(f\"   âœ“ {len([f for f in new_features if '_freq' in f])} frequency features\")\n",
    "    \n",
    "\n",
    "    print(\"\\nðŸ”¨ 6. Statistical aggregation...\")\n",
    "    \n",
    "    if len(numerical_cols) >= 3:\n",
    "        selected_num_cols = numerical_cols[:min(10, len(numerical_cols))]\n",
    "        \n",
    "        try:\n",
    "            df_enh['row_mean'] = df_enh[selected_num_cols].mean(axis=1)\n",
    "            df_enh['row_std'] = df_enh[selected_num_cols].std(axis=1)\n",
    "            df_enh['row_max'] = df_enh[selected_num_cols].max(axis=1)\n",
    "            df_enh['row_min'] = df_enh[selected_num_cols].min(axis=1)\n",
    "            df_enh['row_median'] = df_enh[selected_num_cols].median(axis=1)\n",
    "            df_enh['row_range'] = df_enh['row_max'] - df_enh['row_min']\n",
    "            \n",
    "            new_features.extend(['row_mean', 'row_std', 'row_max', 'row_min', 'row_median', 'row_range'])\n",
    "            print(f\"   âœ“ 6 statistical features\")\n",
    "        except Exception as e:\n",
    "            print(f\"   âš ï¸  Statistical aggregation xato: {str(e)}\")\n",
    "    \n",
    "    print(\"\\nðŸ”¨ 7. Binning features (numerical)...\")\n",
    "    \n",
    "    for col in numerical_cols[:min(5, len(numerical_cols))]:\n",
    "        try:\n",
    "            df_enh[f'{col}_bin'] = pd.qcut(\n",
    "                df_enh[col], q=5, labels=False, duplicates='drop'\n",
    "            )\n",
    "            new_features.append(f'{col}_bin')\n",
    "        except Exception as e:\n",
    "            # Agar qcut ishlamasa, cut ishlatamiz\n",
    "            try:\n",
    "                df_enh[f'{col}_bin'] = pd.cut(\n",
    "                    df_enh[col], bins=5, labels=False, duplicates='drop'\n",
    "                )\n",
    "                new_features.append(f'{col}_bin')\n",
    "            except:\n",
    "                continue\n",
    "    \n",
    "    print(f\"   âœ“ {len([f for f in new_features if '_bin' in f])} binning features\")\n",
    "    \n",
    "    print(\"\\nðŸ”¨ 8. Missing value indicators...\")\n",
    "    \n",
    "    missing_cols = df_enh.columns[df_enh.isnull().any()].tolist()\n",
    "    for col in missing_cols:\n",
    "        df_enh[f'{col}_is_missing'] = df_enh[col].isnull().astype(int)\n",
    "        new_features.append(f'{col}_is_missing')\n",
    "    \n",
    "    print(f\"   âœ“ {len(missing_cols)} missing indicators\")\n",
    "    print(\"\\nðŸ”§ Filling missing values...\")\n",
    "    \n",
    "    # Numerical columns - median bilan\n",
    "    num_cols_in_df = df_enh.select_dtypes(include=[np.number]).columns\n",
    "    df_enh[num_cols_in_df] = df_enh[num_cols_in_df].fillna(df_enh[num_cols_in_df].median())\n",
    "    \n",
    "    # Categorical columns - mode bilan (agar bo'lsa)\n",
    "    cat_cols_in_df = df_enh.select_dtypes(exclude=[np.number]).columns\n",
    "    for col in cat_cols_in_df:\n",
    "        df_enh[col] = df_enh[col].fillna(df_enh[col].mode()[0] if len(df_enh[col].mode()) > 0 else 0)\n",
    "    \n",
    "    print(f\"   âœ“ Missing values filled\")\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"âœ… FEATURE ENGINEERING YAKUNLANDI!\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"   Original features:     {df.shape[1]}\")\n",
    "    print(f\"   New features created:  {len(new_features)}\")\n",
    "    print(f\"   Total features:        {df_enh.shape[1]}\")\n",
    "    print(f\"   - Numerical features:  {len(df_enh.select_dtypes(include=[np.number]).columns)}\")\n",
    "    print(f\"   - Non-numerical:       {len(df_enh.select_dtypes(exclude=[np.number]).columns)}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    return df_enh, new_features, feature_types\n",
    "print(\"ðŸ”¨ Feature engineering...\")\n",
    "feature_types = identify_feature_types(X)\n",
    "print(feature_types)\n",
    "X_engineered, new_features, feature_types = engineer_features(X)\n",
    "\n",
    "X_engineered=X\n",
    "print(f\"âœ“ Original features: {X.shape[1]}\")\n",
    "print(f\"âœ“ Engineered features: {X_engineered.shape[1]}\")\n",
    "\n",
    "# Train/Test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_engineered, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "X_train_split, X_val, y_train_split, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, stratify=y_train, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"âœ“ Train: {X_train_split.shape}\")\n",
    "print(f\"âœ“ Val:   {X_val.shape}\")\n",
    "print(f\"âœ“ Test:  {X_test.shape}\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 2: FEATURE SELECTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Select top K features\n",
    "k_best = min(100, X_train_split.shape[1])\n",
    "print(f\"ðŸŽ¯ Selecting top {k_best} features...\")\n",
    "\n",
    "selector = SelectKBest(score_func=mutual_info_classif, k=k_best)\n",
    "selector.fit(X_train_split, y_train_split)\n",
    "\n",
    "X_train_selected = pd.DataFrame(\n",
    "    selector.transform(X_train_split),\n",
    "    columns=selector.get_feature_names_out(),\n",
    "    index=X_train_split.index\n",
    ")\n",
    "X_val_selected = pd.DataFrame(\n",
    "    selector.transform(X_val),\n",
    "    columns=selector.get_feature_names_out(),\n",
    "    index=X_val.index\n",
    ")\n",
    "X_test_selected = pd.DataFrame(\n",
    "    selector.transform(X_test),\n",
    "    columns=selector.get_feature_names_out(),\n",
    "    index=X_test.index\n",
    ")\n",
    "\n",
    "print(f\"âœ“ Selected features: {X_train_selected.shape[1]}\")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = pd.DataFrame(\n",
    "    scaler.fit_transform(X_train_selected),\n",
    "    columns=X_train_selected.columns,\n",
    "    index=X_train_selected.index\n",
    ")\n",
    "X_val_scaled = pd.DataFrame(\n",
    "    scaler.transform(X_val_selected),\n",
    "    columns=X_val_selected.columns,\n",
    "    index=X_val_selected.index\n",
    ")\n",
    "X_test_scaled = pd.DataFrame(\n",
    "    scaler.transform(X_test_selected),\n",
    "    columns=X_test_selected.columns,\n",
    "    index=X_test_selected.index\n",
    ")\n",
    "\n",
    "print(\"âœ“ Scaling completed!\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 4: TRAINING ENSEMBLE MODELS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Model 1: CatBoost (original data)\n",
    "print(\"\\n1. CatBoost (tuned)...\")\n",
    "model_cb = CatBoostClassifier(\n",
    "    iterations=700,\n",
    "    learning_rate=0.03,\n",
    "    depth=9,\n",
    "    l2_leaf_reg=5,\n",
    "    class_weights=[1, 20],\n",
    "    random_seed=42,\n",
    "    verbose=False\n",
    ")\n",
    "model_cb.fit(X_train_scaled, y_train_split)\n",
    "print(\"   âœ“ Trained\")\n",
    "\n",
    "# Model 2: LightGBM (original data)\n",
    "print(\"\\n2. LightGBM (tuned)...\")\n",
    "model_lgbm = LGBMClassifier(\n",
    "    n_estimators=700,\n",
    "    learning_rate=0.03,\n",
    "    max_depth=9,\n",
    "    num_leaves=50,\n",
    "    class_weight={0: 1, 1: 20},\n",
    "    random_state=42,\n",
    "    verbose=-1\n",
    ")\n",
    "model_lgbm.fit(X_train_scaled, y_train_split)\n",
    "print(\"   âœ“ Trained\")\n",
    "\n",
    "# Model 3: XGBoost (original data)\n",
    "print(\"\\n3. XGBoost (tuned)...\")\n",
    "model_xgb = XGBClassifier(\n",
    "    n_estimators=700,\n",
    "    learning_rate=0.03,\n",
    "    max_depth=9,\n",
    "    scale_pos_weight=20,\n",
    "    random_state=42,\n",
    "    verbosity=0\n",
    ")\n",
    "model_xgb.fit(X_train_scaled, y_train_split)\n",
    "print(\"   âœ“ Trained\")\n",
    "\n",
    "# Model 4: CatBoost with SMOTE\n",
    "print(\"\\n4. CatBoost + SMOTE...\")\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train_scaled, y_train_split)\n",
    "model_smote = CatBoostClassifier(\n",
    "    iterations=500,\n",
    "    learning_rate=0.05,\n",
    "    depth=8,\n",
    "    random_seed=42,\n",
    "    verbose=False\n",
    ")\n",
    "model_smote.fit(X_train_smote, y_train_smote)\n",
    "print(\"   âœ“ Trained\")\n",
    "\n",
    "# Model 5: CatBoost with ADASYN\n",
    "print(\"\\n5. CatBoost + ADASYN...\")\n",
    "adasyn = ADASYN(random_state=42)\n",
    "X_train_adasyn, y_train_adasyn = adasyn.fit_resample(X_train_scaled, y_train_split)\n",
    "model_adasyn = CatBoostClassifier(\n",
    "    iterations=500,\n",
    "    learning_rate=0.05,\n",
    "    depth=8,\n",
    "    random_seed=42,\n",
    "    verbose=False\n",
    ")\n",
    "model_adasyn.fit(X_train_adasyn, y_train_adasyn)\n",
    "print(\"   âœ“ Trained\")\n",
    "\n",
    "# Model 6: Calibrated CatBoost\n",
    "print(\"\\n6. Calibrated CatBoost...\")\n",
    "model_calibrated = CalibratedClassifierCV(model_cb, method='isotonic', cv=5)\n",
    "model_calibrated.fit(X_train_scaled, y_train_split)\n",
    "print(\"   âœ“ Trained\")\n",
    "\n",
    "print(\"\\nâœ… All 6 models trained successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf10403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 5: ENSEMBLE PREDICTION\n",
      "================================================================================\n",
      "âœ“ Optimal ensemble weights:\n",
      "   CatBoost       : 0.167\n",
      "   LightGBM       : 0.167\n",
      "   XGBoost        : 0.167\n",
      "   SMOTE          : 0.167\n",
      "   ADASYN         : 0.167\n",
      "   Calibrated     : 0.167\n",
      "\n",
      "================================================================================\n",
      "STEP 6: OPTIMAL THRESHOLD\n",
      "================================================================================\n",
      "âœ“ Optimal threshold: 0.110\n",
      "\n",
      "================================================================================\n",
      "STEP 7: TEST SET EVALUATION\n",
      "================================================================================\n",
      "\n",
      "ðŸ“Š FINAL TEST METRICS:\n",
      "   ROC-AUC:     0.7850\n",
      "   PR-AUC:      0.2174\n",
      "   Precision:   0.108\n",
      "   Recall:      0.749\n",
      "   F1-Score:    0.188\n",
      "   Threshold:   0.110\n",
      "   False Neg:   231\n",
      "   Total Cost:  $5,165,000\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 5: ENSEMBLE PREDICTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Get predictions from all models\n",
    "predictions_val = [\n",
    "    model_cb.predict_proba(X_val_scaled)[:, 1],\n",
    "    model_lgbm.predict_proba(X_val_scaled)[:, 1],\n",
    "    model_xgb.predict_proba(X_val_scaled)[:, 1],\n",
    "    model_smote.predict_proba(X_val_scaled)[:, 1],\n",
    "    model_adasyn.predict_proba(X_val_scaled)[:, 1],\n",
    "    model_calibrated.predict_proba(X_val_scaled)[:, 1]\n",
    "]\n",
    "\n",
    "predictions_test = [\n",
    "    model_cb.predict_proba(X_test_scaled)[:, 1],\n",
    "    model_lgbm.predict_proba(X_test_scaled)[:, 1],\n",
    "    model_xgb.predict_proba(X_test_scaled)[:, 1],\n",
    "    model_smote.predict_proba(X_test_scaled)[:, 1],\n",
    "    model_adasyn.predict_proba(X_test_scaled)[:, 1],\n",
    "    model_calibrated.predict_proba(X_test_scaled)[:, 1]\n",
    "]\n",
    "# Optimize ensemble weights\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def ensemble_objective(weights):\n",
    "    y_proba = np.average(predictions_val, axis=0, weights=weights)\n",
    "    return -roc_auc_score(y_val, y_proba)\n",
    "\n",
    "constraints = {'type': 'eq', 'fun': lambda w: np.sum(w) - 1}\n",
    "bounds = [(0, 1) for _ in range(6)]\n",
    "initial_weights = [1/6] * 6\n",
    "\n",
    "result = minimize(ensemble_objective, initial_weights, bounds=bounds, constraints=constraints)\n",
    "optimal_weights = result.x\n",
    "\n",
    "print(\"âœ“ Optimal ensemble weights:\")\n",
    "model_names = ['CatBoost', 'LightGBM', 'XGBoost', 'SMOTE', 'ADASYN', 'Calibrated']\n",
    "for name, weight in zip(model_names, optimal_weights):\n",
    "    print(f\"   {name:15s}: {weight:.3f}\")\n",
    "\n",
    "# Final ensemble predictions\n",
    "y_proba_val = np.average(predictions_val, axis=0, weights=optimal_weights)\n",
    "y_proba_test = np.average(predictions_test, axis=0, weights=optimal_weights)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 6: OPTIMAL THRESHOLD\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, average_precision_score\n",
    "\n",
    "thresholds = np.arange(0.1, 0.9, 0.01)\n",
    "costs = []\n",
    "for threshold in thresholds:\n",
    "    y_pred = (y_proba_val >= threshold).astype(int)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_val, y_pred).ravel()\n",
    "    total_cost = (fn * 10000) + (fp * 500)\n",
    "    costs.append(total_cost)\n",
    "\n",
    "optimal_threshold = thresholds[np.argmin(costs)]\n",
    "print(f\"âœ“ Optimal threshold: {optimal_threshold:.3f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 7: TEST SET EVALUATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "roc_auc_test = roc_auc_score(y_test, y_proba_test)\n",
    "pr_auc_test = average_precision_score(y_test, y_proba_test)\n",
    "\n",
    "y_pred_test = (y_proba_test >= optimal_threshold).astype(int)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred_test).ravel()\n",
    "total_cost = (fn * 10000) + (fp * 500)\n",
    "\n",
    "precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "print(f\"\\nðŸ“Š FINAL TEST METRICS:\")\n",
    "print(f\"   ROC-AUC:     {roc_auc_test:.4f}\")\n",
    "print(f\"   PR-AUC:      {pr_auc_test:.4f}\")\n",
    "print(f\"   Precision:   {precision:.3f}\")\n",
    "print(f\"   Recall:      {recall:.3f}\")\n",
    "print(f\"   F1-Score:    {f1:.3f}\")\n",
    "print(f\"   Threshold:   {optimal_threshold:.3f}\")\n",
    "print(f\"   False Neg:   {fn}\")\n",
    "print(f\"   Total Cost:  ${total_cost:,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8374c1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
