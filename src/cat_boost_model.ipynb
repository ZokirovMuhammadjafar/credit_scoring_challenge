{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785c7f40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CATBOOST OPTIMIZATION + CORRELATION ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "\n",
      "üöÄ üöÄ üöÄ üöÄ üöÄ üöÄ üöÄ üöÄ üöÄ üöÄ üöÄ üöÄ üöÄ üöÄ üöÄ üöÄ üöÄ üöÄ üöÄ üöÄ üöÄ üöÄ üöÄ üöÄ üöÄ üöÄ üöÄ üöÄ üöÄ üöÄ üöÄ üöÄ üöÄ üöÄ üöÄ \n",
      "MAIN PIPELINE START\n",
      "üöÄ üöÄ üöÄ üöÄ üöÄ üöÄ üöÄ üöÄ üöÄ üöÄ üöÄ üöÄ üöÄ üöÄ üöÄ üöÄ üöÄ üöÄ üöÄ üöÄ üöÄ üöÄ üöÄ üöÄ üöÄ üöÄ üöÄ üöÄ üöÄ üöÄ üöÄ üöÄ üöÄ üöÄ üöÄ \n",
      "\n",
      "üì¶ STEP 1: Loading Data\n",
      "======================================================================\n",
      "‚úì Train: (57599, 52)\n",
      "‚úì Val:   (14400, 52)\n",
      "‚úì Test:  (18000, 52)\n",
      "\n",
      "üî® STEP 2: Feature Engineering\n",
      "======================================================================\n",
      "‚úì Enhanced features: 81\n",
      "\n",
      "üéØ STEP 3: Feature Selection + Correlation Check\n",
      "======================================================================\n",
      "\n",
      "üéØ FEATURE SELECTION WITH CORRELATION CHECK\n",
      "======================================================================\n",
      "   Max features: 40\n",
      "   Correlation threshold: 0.85\n",
      "\n",
      "1Ô∏è‚É£  Calculating initial feature importance...\n",
      "   ‚úì Initial importance calculated\n",
      "\n",
      "2Ô∏è‚É£  Analyzing correlations...\n",
      "\n",
      "üîç CORRELATION ANALYSIS\n",
      "======================================================================\n",
      "   Threshold: 0.85 (>85% correlation)\n",
      "\n",
      "üìä Found 84 highly correlated pairs (>0.85):\n",
      "                        feature_1                  feature_2  correlation\n",
      "        oldest_account_age_months     oldest_credit_line_age     1.000000\n",
      "           preferred_contact_freq          preferred_contact     1.000000\n",
      "                          row_min        loan_to_value_ratio     1.000000\n",
      "             recent_inquiry_count          num_inquiries_6mo     1.000000\n",
      "        recent_inquiry_count_freq     num_inquiries_6mo_freq     1.000000\n",
      "                 is_referred_freq                is_referred     1.000000\n",
      "              annual_debt_payment total_monthly_debt_payment     1.000000\n",
      "             num_collections_freq            num_collections     1.000000\n",
      "          num_public_records_freq         num_public_records     1.000000\n",
      "           paperless_billing_freq          paperless_billing     1.000000\n",
      "              has_mobile_app_freq             has_mobile_app     1.000000\n",
      "                   monthly_income              annual_income     1.000000\n",
      "                          row_max                    row_std     0.999795\n",
      "      num_delinquencies_2yrs_freq     num_delinquencies_2yrs     0.995657\n",
      "                          row_std                   row_mean     0.992379\n",
      "                          row_max                   row_mean     0.991996\n",
      "loan_amount_x_loan_to_value_ratio                loan_amount     0.987036\n",
      "                          row_std                loan_amount     0.985196\n",
      "                          row_max                loan_amount     0.984832\n",
      "                         row_mean                loan_amount     0.984053\n",
      "\n",
      "3Ô∏è‚É£  Removing correlated features...\n",
      "\n",
      "üóëÔ∏è  REMOVING CORRELATED FEATURES\n",
      "======================================================================\n",
      "   Threshold: 0.85\n",
      "   Calculating feature importance...\n",
      "\n",
      "üìä Results:\n",
      "   Original features:  81\n",
      "   Dropped features:   33\n",
      "   Selected features:  48\n",
      "\n",
      "üóëÔ∏è  Dropped features:\n",
      "      - annual_debt_payment\n",
      "      - annual_income\n",
      "      - available_credit\n",
      "      - has_mobile_app\n",
      "      - housing_price_index\n",
      "      - interest_rate_log\n",
      "      - is_referred_freq\n",
      "      - loan_amount\n",
      "      - loan_amount_log\n",
      "      - loan_amount_x_interest_rate\n",
      "      - loan_amount_x_loan_to_value_ratio\n",
      "      - loan_term\n",
      "      - loan_to_annual_income\n",
      "      - loan_to_value_ratio\n",
      "      - loan_type\n",
      "      ... and 18 more\n",
      "\n",
      "4Ô∏è‚É£  Final selection (top 40)...\n",
      "\n",
      "‚úÖ FINAL SELECTION:\n",
      "   Original:         81 features\n",
      "   After decorr:     48 features\n",
      "   Final selected:   40 features\n",
      "\n",
      "üìä Top 20 Selected Features:\n",
      "                   feature  importance\n",
      "    monthly_free_cash_flow    7.285180\n",
      "                       age    5.634243\n",
      "             interest_rate    4.744772\n",
      "        total_credit_limit    4.664285\n",
      "     existing_monthly_debt    4.604915\n",
      "         employment_length    4.538803\n",
      "   account_diversity_index    4.333908\n",
      "total_monthly_debt_payment    4.094454\n",
      "regional_unemployment_rate    3.990334\n",
      "      cost_of_living_index    3.960619\n",
      "    oldest_credit_line_age    3.822820\n",
      "        credit_utilization    3.710204\n",
      "       num_credit_accounts    3.364614\n",
      "        debt_service_ratio    3.310252\n",
      "       credit_usage_amount    3.264532\n",
      "           monthly_payment    3.071581\n",
      "      regional_median_rent    2.780080\n",
      "         loan_purpose_freq    2.393133\n",
      "        num_login_sessions    2.031550\n",
      "          application_hour    1.721093\n",
      "\n",
      "üìä Visualizing correlations...\n",
      "\n",
      "üìä PLOTTING CORRELATION HEATMAP\n",
      "======================================================================\n",
      "   ‚ö†Ô∏è  Visualization skipped\n",
      "\n",
      "üìè STEP 4: Scaling\n",
      "======================================================================\n",
      "‚úì Scaling completed\n",
      "\n",
      "‚öôÔ∏è  STEP 5: Hyperparameter Tuning\n",
      "======================================================================\n",
      "\n",
      "üîç OPTUNA HYPERPARAMETER OPTIMIZATION\n",
      "======================================================================\n",
      "Running 30 trials...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65adda9c4c49430eb3ef84f902054ab2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, confusion_matrix\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "import optuna\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"CATBOOST OPTIMIZATION + CORRELATION ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ==============================================================================\n",
    "# CORRELATION ANALYSIS FUNCTIONS\n",
    "# ==============================================================================\n",
    "\n",
    "def analyze_correlations(X, threshold=0.85):\n",
    "    \"\"\"\n",
    "    Feature'lar orasidagi kuchli correlation'ni aniqlash\n",
    "    \"\"\"\n",
    "    print(f\"\\nüîç CORRELATION ANALYSIS\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"   Threshold: {threshold} (>{threshold*100:.0f}% correlation)\")\n",
    "    \n",
    "    # Correlation matrix hisoblash\n",
    "    corr_matrix = X.corr().abs()\n",
    "    \n",
    "    # Upper triangle\n",
    "    upper_triangle = corr_matrix.where(\n",
    "        np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)\n",
    "    )\n",
    "    \n",
    "    # Yuqori correlation'li pairlar topish\n",
    "    high_corr_pairs = []\n",
    "    for column in upper_triangle.columns:\n",
    "        high_corr = upper_triangle[column][upper_triangle[column] > threshold]\n",
    "        for idx in high_corr.index:\n",
    "            high_corr_pairs.append({\n",
    "                'feature_1': column,\n",
    "                'feature_2': idx,\n",
    "                'correlation': high_corr[idx]\n",
    "            })\n",
    "    \n",
    "    high_corr_df = pd.DataFrame(high_corr_pairs).sort_values('correlation', ascending=False)\n",
    "    \n",
    "    print(f\"\\nüìä Found {len(high_corr_df)} highly correlated pairs (>{threshold}):\")\n",
    "    if len(high_corr_df) > 0:\n",
    "        print(high_corr_df.head(20).to_string(index=False))\n",
    "    else:\n",
    "        print(\"   ‚úÖ No highly correlated features found!\")\n",
    "    \n",
    "    return high_corr_df, corr_matrix\n",
    "\n",
    "\n",
    "def remove_correlated_features(X, y, threshold=0.85):\n",
    "    \"\"\"\n",
    "    Kuchli correlation'li feature'lardan birini o'chirish\n",
    "    Feature importance asosida kam muhimini o'chiradi\n",
    "    \"\"\"\n",
    "    print(f\"\\nüóëÔ∏è  REMOVING CORRELATED FEATURES\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"   Threshold: {threshold}\")\n",
    "    \n",
    "    # Correlation matrix\n",
    "    corr_matrix = X.corr().abs()\n",
    "    upper_triangle = corr_matrix.where(\n",
    "        np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)\n",
    "    )\n",
    "    \n",
    "    # Feature importance hisoblash\n",
    "    print(\"   Calculating feature importance...\")\n",
    "    temp_model = CatBoostClassifier(\n",
    "        iterations=100, \n",
    "        depth=6,\n",
    "        class_weights=[1, 20],\n",
    "        verbose=False, \n",
    "        random_seed=42\n",
    "    )\n",
    "    temp_model.fit(X, y)\n",
    "    feature_importance = pd.Series(\n",
    "        temp_model.feature_importances_, \n",
    "        index=X.columns\n",
    "    )\n",
    "    \n",
    "    # O'chiriladigan feature'larni topish\n",
    "    to_drop = set()\n",
    "    \n",
    "    for column in upper_triangle.columns:\n",
    "        high_corr = upper_triangle[column][upper_triangle[column] > threshold]\n",
    "        \n",
    "        for idx in high_corr.index:\n",
    "            if column not in to_drop and idx not in to_drop:\n",
    "                # Kam muhimroqni o'chirish\n",
    "                if feature_importance[column] < feature_importance[idx]:\n",
    "                    to_drop.add(column)\n",
    "                else:\n",
    "                    to_drop.add(idx)\n",
    "    \n",
    "    selected_features = [col for col in X.columns if col not in to_drop]\n",
    "    \n",
    "    print(f\"\\nüìä Results:\")\n",
    "    print(f\"   Original features:  {X.shape[1]}\")\n",
    "    print(f\"   Dropped features:   {len(to_drop)}\")\n",
    "    print(f\"   Selected features:  {len(selected_features)}\")\n",
    "    \n",
    "    if len(to_drop) > 0:\n",
    "        print(f\"\\nüóëÔ∏è  Dropped features:\")\n",
    "        for feat in sorted(to_drop)[:15]:\n",
    "            print(f\"      - {feat}\")\n",
    "        if len(to_drop) > 15:\n",
    "            print(f\"      ... and {len(to_drop) - 15} more\")\n",
    "    \n",
    "    return selected_features, list(to_drop)\n",
    "\n",
    "\n",
    "def plot_correlation_heatmap(X, top_n=30):\n",
    "    \"\"\"\n",
    "    Correlation heatmap chizish\n",
    "    \"\"\"\n",
    "    print(f\"\\nüìä PLOTTING CORRELATION HEATMAP\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Agar juda ko'p feature bo'lsa, faqat top N'ni ko'rsatish\n",
    "    if X.shape[1] > top_n:\n",
    "        # Feature importance bo'yicha top N\n",
    "        temp_model = CatBoostClassifier(iterations=50, verbose=False, random_seed=42)\n",
    "        temp_model.fit(X, y)\n",
    "        \n",
    "        feature_importance = pd.Series(\n",
    "            temp_model.feature_importances_, \n",
    "            index=X.columns\n",
    "        ).sort_values(ascending=False)\n",
    "        \n",
    "        top_features = feature_importance.head(top_n).index.tolist()\n",
    "        X_subset = X[top_features]\n",
    "        print(f\"   Showing top {top_n} most important features\")\n",
    "    else:\n",
    "        X_subset = X\n",
    "        print(f\"   Showing all {X.shape[1]} features\")\n",
    "    \n",
    "    # Correlation matrix\n",
    "    corr_matrix = X_subset.corr()\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(14, 12))\n",
    "    \n",
    "    # Heatmap\n",
    "    mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "    sns.heatmap(\n",
    "        corr_matrix, \n",
    "        mask=mask,\n",
    "        annot=False,\n",
    "        fmt='.2f',\n",
    "        cmap='RdYlBu_r',\n",
    "        center=0,\n",
    "        square=True,\n",
    "        linewidths=0.5,\n",
    "        cbar_kws={\"shrink\": 0.8}\n",
    "    )\n",
    "    \n",
    "    plt.title(f'Feature Correlation Heatmap (Top {len(X_subset.columns)} Features)', \n",
    "              fontsize=16, fontweight='bold', pad=20)\n",
    "    plt.xlabel('Features', fontsize=12)\n",
    "    plt.ylabel('Features', fontsize=12)\n",
    "    plt.xticks(rotation=45, ha='right', fontsize=8)\n",
    "    plt.yticks(rotation=0, fontsize=8)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"   ‚úì Heatmap displayed\")\n",
    "\n",
    "# ==============================================================================\n",
    "# FEATURE SELECTION WITH CORRELATION CHECK\n",
    "# ==============================================================================\n",
    "\n",
    "def select_features_with_correlation_check(X, y, max_features=40, corr_threshold=0.85):\n",
    "    \"\"\"\n",
    "    Feature selection with correlation check\n",
    "    \n",
    "    Pipeline:\n",
    "    1. Analyze correlations\n",
    "    2. Remove highly correlated features\n",
    "    3. Select top K by importance\n",
    "    \"\"\"\n",
    "    print(f\"\\nüéØ FEATURE SELECTION WITH CORRELATION CHECK\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"   Max features: {max_features}\")\n",
    "    print(f\"   Correlation threshold: {corr_threshold}\")\n",
    "    \n",
    "    # Step 1: Initial feature importance\n",
    "    print(f\"\\n1Ô∏è‚É£  Calculating initial feature importance...\")\n",
    "    temp_model = CatBoostClassifier(\n",
    "        iterations=200,\n",
    "        learning_rate=0.1,\n",
    "        depth=6,\n",
    "        class_weights=[1, 20],\n",
    "        random_seed=42,\n",
    "        verbose=False\n",
    "    )\n",
    "    temp_model.fit(X, y)\n",
    "    \n",
    "    initial_importance = pd.DataFrame({\n",
    "        'feature': X.columns,\n",
    "        'importance': temp_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(f\"   ‚úì Initial importance calculated\")\n",
    "    \n",
    "    # Step 2: Correlation analysis\n",
    "    print(f\"\\n2Ô∏è‚É£  Analyzing correlations...\")\n",
    "    high_corr_pairs, corr_matrix = analyze_correlations(X, threshold=corr_threshold)\n",
    "    \n",
    "    # Step 3: Remove correlated features\n",
    "    print(f\"\\n3Ô∏è‚É£  Removing correlated features...\")\n",
    "    selected_features, dropped_features = remove_correlated_features(\n",
    "        X, y, threshold=corr_threshold\n",
    "    )\n",
    "    \n",
    "    X_decorrelated = X[selected_features]\n",
    "    \n",
    "    # Step 4: Final selection based on importance\n",
    "    print(f\"\\n4Ô∏è‚É£  Final selection (top {max_features})...\")\n",
    "    \n",
    "    # Re-calculate importance on decorrelated features\n",
    "    temp_model2 = CatBoostClassifier(\n",
    "        iterations=100,\n",
    "        depth=6,\n",
    "        class_weights=[1, 20],\n",
    "        random_seed=42,\n",
    "        verbose=False\n",
    "    )\n",
    "    temp_model2.fit(X_decorrelated, y)\n",
    "    \n",
    "    final_importance = pd.DataFrame({\n",
    "        'feature': X_decorrelated.columns,\n",
    "        'importance': temp_model2.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    final_features = final_importance.head(max_features)['feature'].tolist()\n",
    "    \n",
    "    print(f\"\\n‚úÖ FINAL SELECTION:\")\n",
    "    print(f\"   Original:         {X.shape[1]} features\")\n",
    "    print(f\"   After decorr:     {len(selected_features)} features\")\n",
    "    print(f\"   Final selected:   {len(final_features)} features\")\n",
    "    \n",
    "    print(f\"\\nüìä Top 20 Selected Features:\")\n",
    "    print(final_importance.head(20).to_string(index=False))\n",
    "    \n",
    "    return final_features, final_importance, high_corr_pairs\n",
    "\n",
    "# ==============================================================================\n",
    "# OPTUNA HYPERPARAMETER TUNING\n",
    "# ==============================================================================\n",
    "\n",
    "def optimize_catboost_with_optuna(X_train, y_train, X_val, y_val, n_trials=30):\n",
    "    \"\"\"\n",
    "    Optuna bilan CatBoost'ni optimize qilish\n",
    "    \"\"\"\n",
    "    print(f\"\\nüîç OPTUNA HYPERPARAMETER OPTIMIZATION\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Running {n_trials} trials...\")\n",
    "    \n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            'iterations': trial.suggest_int('iterations', 300, 1000),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1, log=True),\n",
    "            'depth': trial.suggest_int('depth', 4, 8),\n",
    "            'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1, 10),\n",
    "            'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 10, 50),\n",
    "            'border_count': trial.suggest_int('border_count', 32, 255),\n",
    "            'bagging_temperature': trial.suggest_float('bagging_temperature', 0, 1),\n",
    "            'random_strength': trial.suggest_float('random_strength', 0, 10),\n",
    "            'class_weights': [1, trial.suggest_int('class_1_weight', 15, 25)],\n",
    "            'random_seed': 42,\n",
    "            'verbose': False,\n",
    "            'early_stopping_rounds': 50\n",
    "        }\n",
    "        \n",
    "        model = CatBoostClassifier(**params)\n",
    "        model.fit(X_train, y_train, eval_set=(X_val, y_val), verbose=False)\n",
    "        \n",
    "        y_proba = model.predict_proba(X_val)[:, 1]\n",
    "        return roc_auc_score(y_val, y_proba)\n",
    "    \n",
    "    study = optuna.create_study(direction='maximize', study_name='catboost_optimization')\n",
    "    study.optimize(objective, n_trials=n_trials, show_progress_bar=True)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Optimization completed!\")\n",
    "    print(f\"   Best ROC-AUC: {study.best_value:.4f}\")\n",
    "    print(f\"\\nüìä Best Parameters:\")\n",
    "    for key, value in study.best_params.items():\n",
    "        print(f\"   {key:25s}: {value}\")\n",
    "    \n",
    "    return study.best_params, study.best_value\n",
    "\n",
    "# ==============================================================================\n",
    "# CROSS-VALIDATION ENSEMBLE\n",
    "# ==============================================================================\n",
    "\n",
    "def train_catboost_with_cv(X_train, y_train, params, n_folds=5):\n",
    "    \"\"\"\n",
    "    Cross-validation bilan bir nechta model o'rgatish va ensemble qilish\n",
    "    \"\"\"\n",
    "    print(f\"\\nüîÑ CROSS-VALIDATION TRAINING\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Training {n_folds} models with StratifiedKFold...\")\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "    models = []\n",
    "    cv_scores = []\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(X_train, y_train), 1):\n",
    "        X_fold_train = X_train.iloc[train_idx]\n",
    "        y_fold_train = y_train.iloc[train_idx]\n",
    "        X_fold_val = X_train.iloc[val_idx]\n",
    "        y_fold_val = y_train.iloc[val_idx]\n",
    "        \n",
    "        model = CatBoostClassifier(**params, verbose=False)\n",
    "        model.fit(\n",
    "            X_fold_train, y_fold_train,\n",
    "            eval_set=(X_fold_val, y_fold_val),\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        y_proba = model.predict_proba(X_fold_val)[:, 1]\n",
    "        fold_auc = roc_auc_score(y_fold_val, y_proba)\n",
    "        \n",
    "        models.append(model)\n",
    "        cv_scores.append(fold_auc)\n",
    "        \n",
    "        print(f\"   Fold {fold}: ROC-AUC = {fold_auc:.4f}\")\n",
    "    \n",
    "    mean_auc = np.mean(cv_scores)\n",
    "    std_auc = np.std(cv_scores)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Cross-Validation Results:\")\n",
    "    print(f\"   Mean ROC-AUC: {mean_auc:.4f} ¬± {std_auc:.4f}\")\n",
    "    \n",
    "    return models, mean_auc\n",
    "\n",
    "def predict_with_cv_ensemble(models, X):\n",
    "    \"\"\"\n",
    "    CV modellaridan ensemble prediction\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    for model in models:\n",
    "        pred = model.predict_proba(X)[:, 1]\n",
    "        predictions.append(pred)\n",
    "    \n",
    "    # Average predictions\n",
    "    ensemble_pred = np.mean(predictions, axis=0)\n",
    "    return ensemble_pred\n",
    "\n",
    "# ==============================================================================\n",
    "# MINIMAL FEATURE ENGINEERING\n",
    "# ==============================================================================\n",
    "\n",
    "def identify_feature_types(df, categorical_threshold=10):\n",
    "    \"\"\"Feature types aniqlash\"\"\"\n",
    "    numerical_features = []\n",
    "    categorical_features = []\n",
    "    \n",
    "    for col in df.columns:\n",
    "        if df[col].dtype in ['int64', 'float64']:\n",
    "            n_unique = df[col].nunique()\n",
    "            if n_unique > categorical_threshold:\n",
    "                numerical_features.append(col)\n",
    "            else:\n",
    "                categorical_features.append(col)\n",
    "        else:\n",
    "            categorical_features.append(col)\n",
    "    \n",
    "    return {\n",
    "        'numerical': numerical_features,\n",
    "        'categorical': categorical_features\n",
    "    }\n",
    "\n",
    "def engineer_features_minimal(df, feature_types=None):\n",
    "    \"\"\"Minimal feature engineering\"\"\"\n",
    "    df_enh = df.copy()\n",
    "    \n",
    "    if not isinstance(df_enh, pd.DataFrame):\n",
    "        df_enh = pd.DataFrame(df_enh, columns=[f'feature_{i}' for i in range(df_enh.shape[1])])\n",
    "    \n",
    "    if feature_types is None:\n",
    "        feature_types = identify_feature_types(df_enh, categorical_threshold=10)\n",
    "    \n",
    "    numerical_cols = feature_types['numerical']\n",
    "    categorical_cols = feature_types['categorical']\n",
    "    \n",
    "    new_features = []\n",
    "    \n",
    "    # 1. Log transform (top 3)\n",
    "    for col in numerical_cols[:3]:\n",
    "        if (df_enh[col] > 0).all():\n",
    "            df_enh[f'{col}_log'] = np.log1p(df_enh[col])\n",
    "            new_features.append(f'{col}_log')\n",
    "    \n",
    "    # 2. Interactions (2x2)\n",
    "    if len(numerical_cols) >= 2:\n",
    "        for i in range(min(2, len(numerical_cols))):\n",
    "            for j in range(i+1, min(3, len(numerical_cols))):\n",
    "                col1, col2 = numerical_cols[i], numerical_cols[j]\n",
    "                df_enh[f'{col1}_x_{col2}'] = df_enh[col1] * df_enh[col2]\n",
    "                new_features.append(f'{col1}_x_{col2}')\n",
    "    \n",
    "    # 3. Ratios (top 2)\n",
    "    if len(numerical_cols) >= 2:\n",
    "        for i in range(min(2, len(numerical_cols))):\n",
    "            for j in range(i+1, min(3, len(numerical_cols))):\n",
    "                col1, col2 = numerical_cols[i], numerical_cols[j]\n",
    "                if (df_enh[col2] != 0).all():\n",
    "                    df_enh[f'{col1}_div_{col2}'] = df_enh[col1] / (df_enh[col2] + 1e-5)\n",
    "                    new_features.append(f'{col1}_div_{col2}')\n",
    "    \n",
    "    # 4. Frequency encoding (categorical)\n",
    "    for cat_col in categorical_cols:\n",
    "        freq = df_enh[cat_col].value_counts(normalize=True)\n",
    "        df_enh[f'{cat_col}_freq'] = df_enh[cat_col].map(freq)\n",
    "        new_features.append(f'{cat_col}_freq')\n",
    "    \n",
    "    # 5. Row statistics\n",
    "    if len(numerical_cols) >= 3:\n",
    "        selected = numerical_cols[:min(5, len(numerical_cols))]\n",
    "        df_enh['row_mean'] = df_enh[selected].mean(axis=1)\n",
    "        df_enh['row_std'] = df_enh[selected].std(axis=1)\n",
    "        df_enh['row_max'] = df_enh[selected].max(axis=1)\n",
    "        df_enh['row_min'] = df_enh[selected].min(axis=1)\n",
    "        new_features.extend(['row_mean', 'row_std', 'row_max', 'row_min'])\n",
    "    \n",
    "    # Fill missing\n",
    "    df_enh = df_enh.fillna(df_enh.median(numeric_only=True))\n",
    "    \n",
    "    return df_enh, new_features, feature_types\n",
    "\n",
    "# ==============================================================================\n",
    "# MAIN PIPELINE\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n\\n\" + \"üöÄ \"*35)\n",
    "print(\"MAIN PIPELINE START\")\n",
    "print(\"üöÄ \"*35)\n",
    "\n",
    "# STEP 1: Load Data\n",
    "print(f\"\\nüì¶ STEP 1: Loading Data\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "X = pd.read_csv(\"../merge/merged_x.csv\")\n",
    "y = pd.read_csv(\"../merge/merged_y.csv\").squeeze()  # Convert to Series\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "X_train_split, X_val, y_train_split, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, stratify=y_train, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"‚úì Train: {X_train_split.shape}\")\n",
    "print(f\"‚úì Val:   {X_val.shape}\")\n",
    "print(f\"‚úì Test:  {X_test.shape}\")\n",
    "\n",
    "# STEP 2: Feature Engineering\n",
    "print(f\"\\nüî® STEP 2: Feature Engineering\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "X_train_enh, _, feature_types = engineer_features_minimal(X_train_split)\n",
    "X_val_enh = engineer_features_minimal(X_val, feature_types)[0]\n",
    "X_test_enh = engineer_features_minimal(X_test, feature_types)[0]\n",
    "X_full_train_enh = engineer_features_minimal(X_train, feature_types)[0]\n",
    "\n",
    "print(f\"‚úì Enhanced features: {X_train_enh.shape[1]}\")\n",
    "\n",
    "# STEP 3: Feature Selection WITH CORRELATION CHECK\n",
    "print(f\"\\nüéØ STEP 3: Feature Selection + Correlation Check\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "n_features = min(40, X_train_enh.shape[1])\n",
    "final_features, feature_importance, high_corr_pairs = select_features_with_correlation_check(\n",
    "    X_train_enh, y_train_split,\n",
    "    max_features=n_features,\n",
    "    corr_threshold=0.85\n",
    ")\n",
    "\n",
    "X_train_selected = X_train_enh[final_features]\n",
    "X_val_selected = X_val_enh[final_features]\n",
    "X_test_selected = X_test_enh[final_features]\n",
    "X_full_train_selected = X_full_train_enh[final_features]\n",
    "\n",
    "# Visualization (optional - comment out if too slow)\n",
    "print(f\"\\nüìä Visualizing correlations...\")\n",
    "try:\n",
    "    plot_correlation_heatmap(X_train_selected, top_n=30)\n",
    "except:\n",
    "    print(\"   ‚ö†Ô∏è  Visualization skipped\")\n",
    "\n",
    "# STEP 4: Scaling\n",
    "print(f\"\\nüìè STEP 4: Scaling\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = pd.DataFrame(\n",
    "    scaler.fit_transform(X_train_selected),\n",
    "    columns=final_features,\n",
    "    index=X_train_selected.index\n",
    ")\n",
    "X_val_scaled = pd.DataFrame(\n",
    "    scaler.transform(X_val_selected),\n",
    "    columns=final_features,\n",
    "    index=X_val_selected.index\n",
    ")\n",
    "X_test_scaled = pd.DataFrame(\n",
    "    scaler.transform(X_test_selected),\n",
    "    columns=final_features,\n",
    "    index=X_test_selected.index\n",
    ")\n",
    "X_full_train_scaled = pd.DataFrame(\n",
    "    scaler.fit_transform(X_full_train_selected),\n",
    "    columns=final_features\n",
    ")\n",
    "\n",
    "print(f\"‚úì Scaling completed\")\n",
    "\n",
    "# STEP 5: Optuna Optimization\n",
    "print(f\"\\n‚öôÔ∏è  STEP 5: Hyperparameter Tuning\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "n_trials = 30\n",
    "best_params, best_auc = optimize_catboost_with_optuna(\n",
    "    X_train_scaled, y_train_split, X_val_scaled, y_val, n_trials=n_trials\n",
    ")\n",
    "\n",
    "# STEP 6: Cross-Validation Training\n",
    "print(f\"\\nüîÑ STEP 6: Cross-Validation Training\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# STEP 6: Cross-Validation Training\n",
    "print(f\"\\nüîÑ STEP 6: Cross-Validation Training\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "cv_params = {\n",
    "    'iterations': best_params['iterations'],\n",
    "    'learning_rate': best_params['learning_rate'],\n",
    "    'depth': best_params['depth'],\n",
    "    'l2_leaf_reg': best_params['l2_leaf_reg'],\n",
    "    'min_data_in_leaf': best_params['min_data_in_leaf'],\n",
    "    'border_count': best_params['border_count'],\n",
    "    'bagging_temperature': best_params['bagging_temperature'],\n",
    "    'random_strength': best_params['random_strength'],\n",
    "    'class_weights': [1, best_params['class_1_weight']], \n",
    "    'random_seed': 42,\n",
    "    'early_stopping_rounds': 50\n",
    "}\n",
    "\n",
    "cv_models, cv_mean_auc = train_catboost_with_cv(\n",
    "    X_full_train_scaled, y_train, cv_params, n_folds=5\n",
    ")\n",
    "\n",
    "# STEP 7: Test Set Evaluation\n",
    "print(f\"\\nüéØ STEP 7: Test Set Evaluation\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "y_proba_test = predict_with_cv_ensemble(cv_models, X_test_scaled)\n",
    "\n",
    "roc_auc_test = roc_auc_score(y_test, y_proba_test)\n",
    "pr_auc_test = average_precision_score(y_test, y_proba_test)\n",
    "\n",
    "# Optimal threshold\n",
    "thresholds = np.arange(0.1, 0.9, 0.01)\n",
    "costs = []\n",
    "for threshold in thresholds:\n",
    "    y_pred = (y_proba_test >= threshold).astype(int)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "    total_cost = (fn * 10000) + (fp * 500)\n",
    "    costs.append(total_cost)\n",
    "\n",
    "optimal_threshold = thresholds[np.argmin(costs)]\n",
    "y_pred_test = (y_proba_test >= optimal_threshold).astype(int)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred_test).ravel()\n",
    "total_cost = (fn * 10000) + (fp * 500)\n",
    "\n",
    "precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "print(f\"\\nüìä FINAL TEST METRICS:\")\n",
    "print(\"=\"*70)\n",
    "print(f\"   ROC-AUC:     {roc_auc_test:.4f}\")\n",
    "print(f\"   PR-AUC:      {pr_auc_test:.4f}\")\n",
    "print(f\"   Precision:   {precision:.3f}\")\n",
    "print(f\"   Recall:      {recall:.3f}\")\n",
    "print(f\"   F1-Score:    {f1:.3f}\")\n",
    "print(f\"   Threshold:   {optimal_threshold:.3f}\")\n",
    "print(f\"   False Neg:   {fn}\")\n",
    "print(f\"   Total Cost:  ${total_cost:,}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# STEP 8: Comparison\n",
    "print(f\"\\nüìä TAQQOSLASH:\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "comparison = pd.DataFrame({\n",
    "    'Approach': [\n",
    "        'Baseline (Original)',\n",
    "        'Multi-model Ensemble',\n",
    "        'CatBoost + Correlation Fix'\n",
    "    ],\n",
    "    'ROC-AUC': [\n",
    "        0.7963,\n",
    "        0.7912,\n",
    "        roc_auc_test\n",
    "    ],\n",
    "    'Cost': [\n",
    "        '$4,862,500',\n",
    "        '$5,013,500',\n",
    "        f'${total_cost:,}'\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(comparison.to_string(index=False))\n",
    "\n",
    "improvement = (roc_auc_test - 0.7963) * 100\n",
    "print(f\"\\n{'‚úÖ' if improvement > 0 else '‚ùå'} Baseline'dan o'zgarish: {improvement:+.2f}%\")\n",
    "\n",
    "print(f\"\\nüéâ OPTIMIZATION COMPLETED!\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nüìä TOP 15 MOST IMPORTANT FEATURES:\")\n",
    "print(\"=\"*70)\n",
    "print(feature_importance.head(15).to_string(index=False))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95c2253",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(thresholds, costs, linewidth=2)\n",
    "plt.axvline(optimal_threshold, color='red', linestyle='--', label=f\"Optimal Threshold = {optimal_threshold:.3f}\")\n",
    "\n",
    "plt.title(\"Cost vs Threshold Curve\", fontsize=16)\n",
    "plt.xlabel(\"Threshold\", fontsize=14)\n",
    "plt.ylabel(\"Total Cost ($)\", fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ccdf91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_test, y_proba_test)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(fpr, tpr, linewidth=2)\n",
    "plt.plot([0,1],[0,1],'--', label='Random Model')\n",
    "\n",
    "plt.title(\"ROC Curve\", fontsize=16)\n",
    "plt.xlabel(\"False Positive Rate\", fontsize=14)\n",
    "plt.ylabel(\"True Positive Rate\", fontsize=14)\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3b10af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "prec, rec, thr = precision_recall_curve(y_test, y_proba_test)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(rec, prec, linewidth=2)\n",
    "\n",
    "plt.title(\"Precision‚ÄìRecall Curve\", fontsize=16)\n",
    "plt.xlabel(\"Recall\", fontsize=14)\n",
    "plt.ylabel(\"Precision\", fontsize=14)\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
