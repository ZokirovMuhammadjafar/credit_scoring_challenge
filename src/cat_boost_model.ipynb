{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785c7f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, confusion_matrix\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "import optuna\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "def analyze_correlations(X, threshold=0.85):\n",
    "    print(f\"\\nğŸ” CORRELATION ANALYSIS\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"   Threshold: {threshold} (>{threshold*100:.0f}% correlation)\")\n",
    "    \n",
    "    # Correlation matrix hisoblash\n",
    "    corr_matrix = X.corr().abs()\n",
    "    \n",
    "    # Upper triangle\n",
    "    upper_triangle = corr_matrix.where(\n",
    "        np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)\n",
    "    )\n",
    "    \n",
    "    # Yuqori correlation'li pairlar topish\n",
    "    high_corr_pairs = []\n",
    "    for column in upper_triangle.columns:\n",
    "        high_corr = upper_triangle[column][upper_triangle[column] > threshold]\n",
    "        for idx in high_corr.index:\n",
    "            high_corr_pairs.append({\n",
    "                'feature_1': column,\n",
    "                'feature_2': idx,\n",
    "                'correlation': high_corr[idx]\n",
    "            })\n",
    "    \n",
    "    high_corr_df = pd.DataFrame(high_corr_pairs).sort_values('correlation', ascending=False)\n",
    "    \n",
    "    print(f\"\\nğŸ“Š Found {len(high_corr_df)} highly correlated pairs (>{threshold}):\")\n",
    "    if len(high_corr_df) > 0:\n",
    "        print(high_corr_df.head(20).to_string(index=False))\n",
    "    else:\n",
    "        print(\"   âœ… No highly correlated features found!\")\n",
    "    \n",
    "    return high_corr_df, corr_matrix\n",
    "\n",
    "\n",
    "def remove_correlated_features(X, y, threshold=0.85):\n",
    "    print(f\"\\nğŸ—‘ï¸  REMOVING CORRELATED FEATURES\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"   Threshold: {threshold}\")\n",
    "    \n",
    "    # Correlation matrix\n",
    "    corr_matrix = X.corr().abs()\n",
    "    upper_triangle = corr_matrix.where(\n",
    "        np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)\n",
    "    )\n",
    "    \n",
    "    # Feature importance hisoblash\n",
    "    print(\"   Calculating feature importance...\")\n",
    "    temp_model = CatBoostClassifier(\n",
    "        iterations=100, \n",
    "        depth=6,\n",
    "        class_weights=[1, 20],\n",
    "        verbose=False, \n",
    "        random_seed=42\n",
    "    )\n",
    "    temp_model.fit(X, y)\n",
    "    feature_importance = pd.Series(\n",
    "        temp_model.feature_importances_, \n",
    "        index=X.columns\n",
    "    )\n",
    "    \n",
    "    # O'chiriladigan feature'larni topish\n",
    "    to_drop = set()\n",
    "    \n",
    "    for column in upper_triangle.columns:\n",
    "        high_corr = upper_triangle[column][upper_triangle[column] > threshold]\n",
    "        \n",
    "        for idx in high_corr.index:\n",
    "            if column not in to_drop and idx not in to_drop:\n",
    "                # Kam muhimroqni o'chirish\n",
    "                if feature_importance[column] < feature_importance[idx]:\n",
    "                    to_drop.add(column)\n",
    "                else:\n",
    "                    to_drop.add(idx)\n",
    "    \n",
    "    selected_features = [col for col in X.columns if col not in to_drop]\n",
    "    \n",
    "    print(f\"\\nğŸ“Š Results:\")\n",
    "    print(f\"   Original features:  {X.shape[1]}\")\n",
    "    print(f\"   Dropped features:   {len(to_drop)}\")\n",
    "    print(f\"   Selected features:  {len(selected_features)}\")\n",
    "    \n",
    "    if len(to_drop) > 0:\n",
    "        print(f\"\\nğŸ—‘ï¸  Dropped features:\")\n",
    "        for feat in sorted(to_drop)[:15]:\n",
    "            print(f\"      - {feat}\")\n",
    "        if len(to_drop) > 15:\n",
    "            print(f\"      ... and {len(to_drop) - 15} more\")\n",
    "    \n",
    "    return selected_features, list(to_drop)\n",
    "\n",
    "\n",
    "def plot_correlation_heatmap(X,y, top_n=30):\n",
    "    # Agar juda ko'p feature bo'lsa, faqat top N'ni ko'rsatish\n",
    "    if X.shape[1] > top_n:\n",
    "        # Feature importance bo'yicha top N\n",
    "        temp_model = CatBoostClassifier(iterations=50, verbose=False, random_seed=42)\n",
    "        temp_model.fit(X, y)\n",
    "        \n",
    "        feature_importance = pd.Series(\n",
    "            temp_model.feature_importances_, \n",
    "            index=X.columns\n",
    "        ).sort_values(ascending=False)\n",
    "        \n",
    "        top_features = feature_importance.head(top_n).index.tolist()\n",
    "        X_subset = X[top_features]\n",
    "        print(f\"   Showing top {top_n} most important features\")\n",
    "    else:\n",
    "        X_subset = X\n",
    "        print(f\"   Showing all {X.shape[1]} features\")\n",
    "    \n",
    "    # Correlation matrix\n",
    "    corr_matrix = X_subset.corr()\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(14, 12))\n",
    "    \n",
    "    # Heatmap\n",
    "    mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "    sns.heatmap(\n",
    "        corr_matrix, \n",
    "        mask=mask,\n",
    "        annot=False,\n",
    "        fmt='.2f',\n",
    "        cmap='RdYlBu_r',\n",
    "        center=0,\n",
    "        square=True,\n",
    "        linewidths=0.5,\n",
    "        cbar_kws={\"shrink\": 0.8}\n",
    "    )\n",
    "    \n",
    "    plt.title(f'Feature Correlation Heatmap (Top {len(X_subset.columns)} Features)', \n",
    "              fontsize=16, fontweight='bold', pad=20)\n",
    "    plt.xlabel('Features', fontsize=12)\n",
    "    plt.ylabel('Features', fontsize=12)\n",
    "    plt.xticks(rotation=45, ha='right', fontsize=8)\n",
    "    plt.yticks(rotation=0, fontsize=8)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"   âœ“ Heatmap displayed\")\n",
    "\n",
    "def select_features_with_correlation_check(X, y, max_features=40, corr_threshold=0.85):\n",
    "    print(f\"   Max features: {max_features}\")\n",
    "    print(f\"   Correlation threshold: {corr_threshold}\")\n",
    "    \n",
    "    # Step 1: Initial feature importance\n",
    "    print(f\"\\n1ï¸âƒ£  Calculating initial feature importance...\")\n",
    "    temp_model = CatBoostClassifier(\n",
    "        iterations=200,\n",
    "        learning_rate=0.1,\n",
    "        depth=6,\n",
    "        class_weights=[1, 20],\n",
    "        random_seed=42,\n",
    "        verbose=False\n",
    "    )\n",
    "    temp_model.fit(X, y)\n",
    "    \n",
    "    initial_importance = pd.DataFrame({\n",
    "        'feature': X.columns,\n",
    "        'importance': temp_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(f\"   âœ“ Initial importance calculated\")\n",
    "    \n",
    "    # Step 2: Correlation analysis\n",
    "    print(f\"\\n2ï¸âƒ£  Analyzing correlations...\")\n",
    "    high_corr_pairs, corr_matrix = analyze_correlations(X, threshold=corr_threshold)\n",
    "    \n",
    "    # Step 3: Remove correlated features\n",
    "    print(f\"\\n3ï¸âƒ£  Removing correlated features...\")\n",
    "    selected_features, dropped_features = remove_correlated_features(\n",
    "        X, y, threshold=corr_threshold\n",
    "    )\n",
    "    \n",
    "    X_decorrelated = X[selected_features]\n",
    "    \n",
    "    # Step 4: Final selection based on importance\n",
    "    print(f\"\\n4ï¸âƒ£  Final selection (top {max_features})...\")\n",
    "    \n",
    "    # Re-calculate importance on decorrelated features\n",
    "    temp_model2 = CatBoostClassifier(\n",
    "        iterations=100,\n",
    "        depth=6,\n",
    "        class_weights=[1, 20],\n",
    "        random_seed=42,\n",
    "        verbose=False\n",
    "    )\n",
    "    temp_model2.fit(X_decorrelated, y)\n",
    "    \n",
    "    final_importance = pd.DataFrame({\n",
    "        'feature': X_decorrelated.columns,\n",
    "        'importance': temp_model2.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    final_features = final_importance.head(max_features)['feature'].tolist()\n",
    "    \n",
    "    print(f\"\\nâœ… FINAL SELECTION:\")\n",
    "    print(f\"   Original:         {X.shape[1]} features\")\n",
    "    print(f\"   After decorr:     {len(selected_features)} features\")\n",
    "    print(f\"   Final selected:   {len(final_features)} features\")\n",
    "    \n",
    "    print(f\"\\nğŸ“Š Top 20 Selected Features:\")\n",
    "    print(final_importance.head(20).to_string(index=False))\n",
    "    \n",
    "    return final_features, final_importance, high_corr_pairs\n",
    "\n",
    "def optimize_catboost_with_optuna(X_train, y_train, X_val, y_val, n_trials=30):\n",
    "    print(f\"Running {n_trials} trials...\")\n",
    "    \n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            'iterations': trial.suggest_int('iterations', 300, 1000),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1, log=True),\n",
    "            'depth': trial.suggest_int('depth', 4, 8),\n",
    "            'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1, 10),\n",
    "            'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 10, 50),\n",
    "            'border_count': trial.suggest_int('border_count', 32, 255),\n",
    "            'bagging_temperature': trial.suggest_float('bagging_temperature', 0, 1),\n",
    "            'random_strength': trial.suggest_float('random_strength', 0, 10),\n",
    "            'class_weights': [1, trial.suggest_int('class_1_weight', 15, 25)],\n",
    "            'random_seed': 42,\n",
    "            'verbose': False,\n",
    "            'early_stopping_rounds': 50\n",
    "        }\n",
    "        \n",
    "        model = CatBoostClassifier(**params)\n",
    "        model.fit(X_train, y_train, eval_set=(X_val, y_val), verbose=False)\n",
    "        \n",
    "        y_proba = model.predict_proba(X_val)[:, 1]\n",
    "        return float(roc_auc_score(y_val, y_proba))\n",
    "    \n",
    "    study = optuna.create_study(direction='maximize', study_name='catboost_optimization')\n",
    "    study.optimize(objective, n_trials=n_trials, show_progress_bar=True)\n",
    "    \n",
    "    print(f\"\\nâœ… Optimization completed!\")\n",
    "    print(f\"   Best ROC-AUC: {study.best_value:.4f}\")\n",
    "    print(f\"\\nğŸ“Š Best Parameters:\")\n",
    "    for key, value in study.best_params.items():\n",
    "        print(f\"   {key:25s}: {value}\")\n",
    "    \n",
    "    return study.best_params, study.best_value\n",
    "\n",
    "# ==============================================================================\n",
    "# CROSS-VALIDATION ENSEMBLE\n",
    "# ==============================================================================\n",
    "\n",
    "def train_catboost_with_cv(X_train, y_train, params, n_folds=5):\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "    models = []\n",
    "    cv_scores = []\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(X_train, y_train), 1):\n",
    "        X_fold_train = X_train.iloc[train_idx]\n",
    "        y_fold_train = y_train.iloc[train_idx]\n",
    "        X_fold_val = X_train.iloc[val_idx]\n",
    "        y_fold_val = y_train.iloc[val_idx]\n",
    "        \n",
    "        model = CatBoostClassifier(**params, verbose=False)\n",
    "        model.fit(\n",
    "            X_fold_train, y_fold_train,\n",
    "            eval_set=(X_fold_val, y_fold_val),\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        y_proba = model.predict_proba(X_fold_val)[:, 1]\n",
    "        fold_auc = roc_auc_score(y_fold_val, y_proba)\n",
    "        \n",
    "        models.append(model)\n",
    "        cv_scores.append(fold_auc)\n",
    "        \n",
    "        print(f\"   Fold {fold}: ROC-AUC = {fold_auc:.4f}\")\n",
    "    \n",
    "    mean_auc = np.mean(cv_scores)\n",
    "    std_auc = np.std(cv_scores)\n",
    "    \n",
    "    print(f\"\\nâœ… Cross-Validation Results:\")\n",
    "    print(f\"   Mean ROC-AUC: {mean_auc:.4f} Â± {std_auc:.4f}\")\n",
    "    \n",
    "    return models, mean_auc\n",
    "\n",
    "def predict_with_cv_ensemble(models, X):\n",
    "\n",
    "    predictions = []\n",
    "    for model in models:\n",
    "        pred = model.predict_proba(X)[:, 1]\n",
    "        predictions.append(pred)\n",
    "    \n",
    "    # Average predictions\n",
    "    ensemble_pred = np.mean(predictions, axis=0)\n",
    "    return ensemble_pred\n",
    "\n",
    "def identify_feature_types(df, categorical_threshold=10):\n",
    "    \"\"\"Feature types aniqlash\"\"\"\n",
    "    numerical_features = []\n",
    "    categorical_features = []\n",
    "    \n",
    "    for col in df.columns:\n",
    "        if df[col].dtype in ['int64', 'float64']:\n",
    "            n_unique = df[col].nunique()\n",
    "            if n_unique > categorical_threshold:\n",
    "                numerical_features.append(col)\n",
    "            else:\n",
    "                categorical_features.append(col)\n",
    "        else:\n",
    "            categorical_features.append(col)\n",
    "    \n",
    "    return {\n",
    "        'numerical': numerical_features,\n",
    "        'categorical': categorical_features\n",
    "    }\n",
    "\n",
    "def engineer_features_minimal(df, feature_types=None):\n",
    "    \"\"\"Minimal feature engineering\"\"\"\n",
    "    df_enh = df.copy()\n",
    "    \n",
    "    if not isinstance(df_enh, pd.DataFrame):\n",
    "        df_enh = pd.DataFrame(df_enh, columns=[f'feature_{i}' for i in range(df_enh.shape[1])])\n",
    "    \n",
    "    if feature_types is None:\n",
    "        feature_types = identify_feature_types(df_enh, categorical_threshold=10)\n",
    "    \n",
    "    numerical_cols = feature_types['numerical']\n",
    "    categorical_cols = feature_types['categorical']\n",
    "    \n",
    "    new_features = []\n",
    "    \n",
    "    # 1. Log transform (top 3)\n",
    "    for col in numerical_cols[:3]:\n",
    "        if (df_enh[col] > 0).all():\n",
    "            df_enh[f'{col}_log'] = np.log1p(df_enh[col])\n",
    "            new_features.append(f'{col}_log')\n",
    "    \n",
    "    # 2. Interactions (2x2)\n",
    "    if len(numerical_cols) >= 2:\n",
    "        for i in range(min(2, len(numerical_cols))):\n",
    "            for j in range(i+1, min(3, len(numerical_cols))):\n",
    "                col1, col2 = numerical_cols[i], numerical_cols[j]\n",
    "                df_enh[f'{col1}_x_{col2}'] = df_enh[col1] * df_enh[col2]\n",
    "                new_features.append(f'{col1}_x_{col2}')\n",
    "    \n",
    "    # 3. Ratios (top 2)\n",
    "    if len(numerical_cols) >= 2:\n",
    "        for i in range(min(2, len(numerical_cols))):\n",
    "            for j in range(i+1, min(3, len(numerical_cols))):\n",
    "                col1, col2 = numerical_cols[i], numerical_cols[j]\n",
    "                if (df_enh[col2] != 0).all():\n",
    "                    df_enh[f'{col1}_div_{col2}'] = df_enh[col1] / (df_enh[col2] + 1e-5)\n",
    "                    new_features.append(f'{col1}_div_{col2}')\n",
    "    \n",
    "    # 4. Frequency encoding (categorical)\n",
    "    for cat_col in categorical_cols:\n",
    "        freq = df_enh[cat_col].value_counts(normalize=True)\n",
    "        df_enh[f'{cat_col}_freq'] = df_enh[cat_col].map(freq)\n",
    "        new_features.append(f'{cat_col}_freq')\n",
    "    \n",
    "    # 5. Row statistics\n",
    "    if len(numerical_cols) >= 3:\n",
    "        selected = numerical_cols[:min(5, len(numerical_cols))]\n",
    "        df_enh['row_mean'] = df_enh[selected].mean(axis=1)\n",
    "        df_enh['row_std'] = df_enh[selected].std(axis=1)\n",
    "        df_enh['row_max'] = df_enh[selected].max(axis=1)\n",
    "        df_enh['row_min'] = df_enh[selected].min(axis=1)\n",
    "        new_features.extend(['row_mean', 'row_std', 'row_max', 'row_min'])\n",
    "    \n",
    "    # Fill missing\n",
    "    df_enh = df_enh.fillna(df_enh.median(numeric_only=True))\n",
    "    \n",
    "    return df_enh, new_features, feature_types\n",
    "\n",
    "# STEP 1: Load Data\n",
    "print(f\"\\nğŸ“¦ STEP 1: Loading Data\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "X = pd.read_csv(\"../merge/merged_x.csv\")\n",
    "y = pd.read_csv(\"../merge/merged_y.csv\").squeeze()  # Convert to Series\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, stratify=y, random_state=42\n",
    ")\n",
    "X_train_split, X_val, y_train_split, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.28, stratify=y_train, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"âœ“ Train: {X_train_split.shape}\")\n",
    "print(f\"âœ“ Val:   {X_val.shape}\")\n",
    "print(f\"âœ“ Test:  {X_test.shape}\")\n",
    "\n",
    "# STEP 2: Feature Engineering\n",
    "print(f\"\\nğŸ”¨ STEP 2: Feature Engineering\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "X_train_enh, _, feature_types = engineer_features_minimal(X_train_split)\n",
    "X_val_enh = engineer_features_minimal(X_val, feature_types)[0]\n",
    "X_test_enh = engineer_features_minimal(X_test, feature_types)[0]\n",
    "X_full_train_enh = engineer_features_minimal(X_train, feature_types)[0]\n",
    "\n",
    "print(f\"âœ“ Enhanced features: {X_train_enh.shape[1]}\")\n",
    "\n",
    "# STEP 3: Feature Selection WITH CORRELATION CHECK\n",
    "print(f\"\\nğŸ¯ STEP 3: Feature Selection + Correlation Check\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "n_features = min(40, X_train_enh.shape[1])\n",
    "final_features, feature_importance, high_corr_pairs = select_features_with_correlation_check(\n",
    "    X_train_enh, y_train_split,\n",
    "    max_features=n_features,\n",
    "    corr_threshold=0.85\n",
    ")\n",
    "\n",
    "X_train_selected = X_train_enh[final_features]\n",
    "X_val_selected = X_val_enh[final_features]\n",
    "X_test_selected = X_test_enh[final_features]\n",
    "X_full_train_selected = X_full_train_enh[final_features]\n",
    "\n",
    "# Visualization (optional - comment out if too slow)\n",
    "print(f\"\\nğŸ“Š Visualizing correlations...\")\n",
    "try:\n",
    "    plot_correlation_heatmap(X_train_selected,y_train, top_n=30)\n",
    "except:\n",
    "    print(\"   âš ï¸  Visualization skipped\")\n",
    "\n",
    "# STEP 4: Scaling\n",
    "print(f\"\\nğŸ“ STEP 4: Scaling\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = pd.DataFrame(\n",
    "    scaler.fit_transform(X_train_selected),\n",
    "    columns=final_features,\n",
    "    index=X_train_selected.index\n",
    ")\n",
    "X_val_scaled = pd.DataFrame(\n",
    "    scaler.transform(X_val_selected),\n",
    "    columns=final_features,\n",
    "    index=X_val_selected.index\n",
    ")\n",
    "X_test_scaled = pd.DataFrame(\n",
    "    scaler.transform(X_test_selected),\n",
    "    columns=final_features,\n",
    "    index=X_test_selected.index\n",
    ")\n",
    "X_full_train_scaled = pd.DataFrame(\n",
    "    scaler.fit_transform(X_full_train_selected),\n",
    "    columns=final_features\n",
    ")\n",
    "\n",
    "print(f\"âœ“ Scaling completed\")\n",
    "\n",
    "# STEP 5: Optuna Optimization\n",
    "print(f\"\\nâš™ï¸  STEP 5: Hyperparameter Tuning\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "n_trials = 30\n",
    "best_params, best_auc = optimize_catboost_with_optuna(\n",
    "    X_train_scaled, y_train_split, X_val_scaled, y_val, n_trials=n_trials\n",
    ")\n",
    "\n",
    "# STEP 6: Cross-Validation Training\n",
    "print(f\"\\nğŸ”„ STEP 6: Cross-Validation Training\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# STEP 6: Cross-Validation Training\n",
    "print(f\"\\nğŸ”„ STEP 6: Cross-Validation Training\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "cv_params = {\n",
    "    'iterations': best_params['iterations'],\n",
    "    'learning_rate': best_params['learning_rate'],\n",
    "    'depth': best_params['depth'],\n",
    "    'l2_leaf_reg': best_params['l2_leaf_reg'],\n",
    "    'min_data_in_leaf': best_params['min_data_in_leaf'],\n",
    "    'border_count': best_params['border_count'],\n",
    "    'bagging_temperature': best_params['bagging_temperature'],\n",
    "    'random_strength': best_params['random_strength'],\n",
    "    'class_weights': [1, best_params['class_1_weight']], \n",
    "    'random_seed': 42,\n",
    "    'early_stopping_rounds': 50\n",
    "}\n",
    "\n",
    "cv_models, cv_mean_auc = train_catboost_with_cv(\n",
    "    X_full_train_scaled, y_train, cv_params, n_folds=5\n",
    ")\n",
    "\n",
    "# STEP 7: Test Set Evaluation\n",
    "print(f\"\\nğŸ¯ STEP 7: Test Set Evaluation\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "y_proba_test = predict_with_cv_ensemble(cv_models, X_test_scaled)\n",
    "\n",
    "roc_auc_test = roc_auc_score(y_test, y_proba_test)\n",
    "pr_auc_test = average_precision_score(y_test, y_proba_test)\n",
    "\n",
    "# Optimal threshold\n",
    "thresholds = np.arange(0.1, 0.9, 0.01)\n",
    "costs = []\n",
    "for threshold in thresholds:\n",
    "    y_pred = (y_proba_test >= threshold).astype(int)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "    total_cost = (fn * 10000) + (fp * 500)\n",
    "    costs.append(total_cost)\n",
    "\n",
    "optimal_threshold = thresholds[np.argmin(costs)]\n",
    "y_pred_test = (y_proba_test >= optimal_threshold).astype(int)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred_test).ravel()\n",
    "total_cost = (fn * 10000) + (fp * 500)\n",
    "\n",
    "precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "print(f\"\\nğŸ“Š FINAL TEST METRICS:\")\n",
    "print(\"=\"*70)\n",
    "print(f\"   ROC-AUC:     {roc_auc_test:.4f}\")\n",
    "print(f\"   PR-AUC:      {pr_auc_test:.4f}\")\n",
    "print(f\"   Precision:   {precision:.3f}\")\n",
    "print(f\"   Recall:      {recall:.3f}\")\n",
    "print(f\"   F1-Score:    {f1:.3f}\")\n",
    "print(f\"   Threshold:   {optimal_threshold:.3f}\")\n",
    "print(f\"   False Neg:   {fn}\")\n",
    "print(f\"   Total Cost:  ${total_cost:,}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# STEP 8: Comparison\n",
    "print(f\"\\nğŸ“Š TAQQOSLASH:\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "comparison = pd.DataFrame({\n",
    "    'Approach': [\n",
    "        'Baseline (Original)',\n",
    "        'Multi-model Ensemble',\n",
    "        'CatBoost + Correlation Fix'\n",
    "    ],\n",
    "    'ROC-AUC': [\n",
    "        0.7963,\n",
    "        0.7912,\n",
    "        roc_auc_test\n",
    "    ],\n",
    "    'Cost': [\n",
    "        '$4,862,500',\n",
    "        '$5,013,500',\n",
    "        f'${total_cost:,}'\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(comparison.to_string(index=False))\n",
    "\n",
    "improvement = (roc_auc_test - 0.7963) * 100\n",
    "print(f\"\\n{'âœ…' if improvement > 0 else 'âŒ'} Baseline'dan o'zgarish: {improvement:+.2f}%\")\n",
    "\n",
    "print(f\"\\nğŸ‰ OPTIMIZATION COMPLETED!\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nğŸ“Š TOP 15 MOST IMPORTANT FEATURES:\")\n",
    "print(\"=\"*70)\n",
    "print(feature_importance.head(15).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95c2253",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(thresholds, costs, linewidth=2)\n",
    "plt.axvline(optimal_threshold, color='red', linestyle='--', label=f\"Optimal Threshold = {optimal_threshold:.3f}\")\n",
    "\n",
    "plt.title(\"Cost vs Threshold Curve\", fontsize=16)\n",
    "plt.xlabel(\"Threshold\", fontsize=14)\n",
    "plt.ylabel(\"Total Cost ($)\", fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ccdf91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_test, y_proba_test)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(fpr, tpr, linewidth=2)\n",
    "plt.plot([0,1],[0,1],'--', label='Random Model')\n",
    "\n",
    "plt.title(\"ROC Curve\", fontsize=16)\n",
    "plt.xlabel(\"False Positive Rate\", fontsize=14)\n",
    "plt.ylabel(\"True Positive Rate\", fontsize=14)\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3b10af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "prec, rec, thr = precision_recall_curve(y_test, y_proba_test)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(rec, prec, linewidth=2)\n",
    "\n",
    "plt.title(\"Precisionâ€“Recall Curve\", fontsize=16)\n",
    "plt.xlabel(\"Recall\", fontsize=14)\n",
    "plt.ylabel(\"Precision\", fontsize=14)\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c7f593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# VISUALIZATIONS FOR PRESENTATION\n",
    "# ==============================================================================\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, precision_recall_curve\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')  # Clean presentation style\n",
    "\n",
    "# 1ï¸âƒ£ Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred_test)\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.imshow(cm, cmap='Blues')\n",
    "plt.title(\"Confusion Matrix (Threshold = {:.3f})\".format(optimal_threshold), fontsize=16)\n",
    "plt.colorbar()\n",
    "\n",
    "plt.xticks([0,1], [\"Pred 0\", \"Pred 1\"], fontsize=12)\n",
    "plt.yticks([0,1], [\"True 0\", \"True 1\"], fontsize=12)\n",
    "\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        plt.text(j, i, cm[i,j], ha='center', va='center', fontsize=14, color=\"black\")\n",
    "\n",
    "plt.xlabel(\"Predicted\", fontsize=14)\n",
    "plt.ylabel(\"Actual\", fontsize=14)\n",
    "plt.show()\n",
    "\n",
    "top15 = feature_importance.head(60)\n",
    "\n",
    "plt.figure(figsize=(24,10))\n",
    "plt.barh(top15['feature'], top15['importance'], color='skyblue')\n",
    "plt.gca().invert_yaxis()\n",
    "\n",
    "plt.title(\"Top 15 Feature Importances\", fontsize=16)\n",
    "plt.xlabel(\"Importance Score\", fontsize=14)\n",
    "plt.ylabel(\"Feature\", fontsize=14)\n",
    "plt.grid(axis='x')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4507eadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_data = pd.read_csv(\"../test/merged_x.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f1b045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "REAL DATA PREDICTION\n",
      "================================================================================\n",
      "\n",
      "    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
      "    â•‘         REAL DATA PREDICTION - USAGE EXAMPLE               â•‘\n",
      "    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "    \n",
      "    VARIANT 1: Agar model hali xotirada bo'lsa (training'dan keyim)\n",
      "    ================================================================\n",
      "    \n",
      "    result = predict_real_data(\n",
      "        real_data_path=\"real_data.csv\",\n",
      "        cv_models=cv_models,              # Training'dan\n",
      "        scaler=scaler,                     # Training'dan\n",
      "        final_features=final_features,     # Training'dan\n",
      "        optimal_threshold=optimal_threshold, # Training'dan\n",
      "        feature_types=feature_types,       # Training'dan\n",
      "        extra_column_name=\"customer_id\",   # Agar ortiqcha column bo'lsa\n",
      "        output_path=\"predictions.csv\"\n",
      "    )\n",
      "    \n",
      "    \n",
      "    VARIANT 2: Agar model pickle'da saqlangan bo'lsa\n",
      "    =================================================\n",
      "    \n",
      "    import pickle\n",
      "    \n",
      "    # Load model\n",
      "    with open('best_catboost_with_correlation.pkl', 'rb') as f:\n",
      "        data = pickle.load(f)\n",
      "    \n",
      "    # Predict\n",
      "    result = predict_real_data(\n",
      "        real_data_path=\"real_data.csv\",\n",
      "        cv_models=data['models'],\n",
      "        scaler=data['scaler'],\n",
      "        final_features=data['selected_features'],\n",
      "        optimal_threshold=data['optimal_threshold'],\n",
      "        feature_types=data['feature_types'],\n",
      "        extra_column_name=None,  # Agar yo'q bo'lsa None\n",
      "        output_path=\"predictions.csv\"\n",
      "    )\n",
      "    \n",
      "    # Results\n",
      "    print(result.head(10))\n",
      "    \n",
      "\n",
      "======================================================================\n",
      "STARTING PREDICTION PIPELINE\n",
      "======================================================================\n",
      "\n",
      "ğŸ“‚ Loading real data...\n",
      "âœ… Loaded: (10001, 41)\n",
      "   Columns: ['customer_id', 'loan_type', 'loan_amount', 'loan_term', 'interest_rate', 'loan_purpose', 'loan_to_value_ratio', 'origination_channel', 'age', 'annual_income', 'employment_length', 'employment_type', 'education', 'marital_status', 'num_dependents', 'regional_unemployment_rate', 'regional_median_income', 'regional_median_rent', 'housing_price_index', 'oldest_credit_line_age', 'total_credit_limit', 'num_delinquencies_2yrs', 'num_inquiries_6mo', 'recent_inquiry_count', 'num_public_records', 'num_collections', 'account_diversity_index', 'num_customer_service_calls', 'paperless_billing', 'account_tenure', 'monthly_income', 'existing_monthly_debt', 'monthly_payment', 'debt_service_ratio', 'payment_to_income_ratio', 'credit_utilization', 'available_credit', 'annual_debt_payment', 'loan_to_annual_income', 'total_debt_amount', 'monthly_free_cash_flow']\n",
      "\n",
      "ğŸ”§ No extra column to remove\n",
      "âœ… Clean data: (10001, 41)\n",
      "\n",
      "ğŸ”¨ Feature engineering...\n",
      "âœ… Engineered: 65 features\n",
      "\n",
      "ğŸ¯ Selecting required features...\n",
      "âœ… Selected: 36 features\n",
      "\n",
      "ğŸ“ Scaling features...\n",
      "âœ… Scaled!\n",
      "\n",
      "ğŸ”® Making predictions...\n",
      "âœ… Predictions done!\n",
      "   Total: 10001\n",
      "   Class 0 (Approve): 7235 (72.3%)\n",
      "   Class 1 (Reject):  2766 (27.7%)\n",
      "\n",
      "ğŸ“Š Creating results...\n",
      "âœ… Saved to: predictions.csv\n",
      "\n",
      "======================================================================\n",
      "PREDICTION SUMMARY\n",
      "======================================================================\n",
      "\n",
      "ğŸ¯ Threshold: 0.490\n",
      "\n",
      "ğŸ“Š Predictions:\n",
      "prediction\n",
      "0    7235\n",
      "1    2766\n",
      "\n",
      "ğŸ“Š Risk Levels:\n",
      "risk_level\n",
      "Low          4492\n",
      "Medium       2859\n",
      "High         1713\n",
      "Very High     937\n",
      "\n",
      "ğŸ“Š Score Statistics:\n",
      "   Min:    0.0263\n",
      "   Max:    0.9659\n",
      "   Mean:   0.3696\n",
      "   Median: 0.3280\n",
      "\n",
      "âœ… COMPLETED!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "REAL DATA PREDICTION - SIMPLE VERSION\n",
    "======================================\n",
    "Allaqachon o'rgatilgan model bilan real data'ni predict qilish\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"REAL DATA PREDICTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ==============================================================================\n",
    "# SIZNING MAVJUD MODELINGIZ\n",
    "# ==============================================================================\n",
    "# cv_models - sizning trained modellaringiz\n",
    "# scaler - sizning scaler'ingiz\n",
    "# selected_features / final_features - feature list\n",
    "# optimal_threshold - threshold\n",
    "# feature_types - feature types dict\n",
    "\n",
    "# Agar bu o'zgaruvchilar hali mavjud bo'lmasa, ularni pickle'dan yuklang:\n",
    "# import pickle\n",
    "# with open('best_catboost_with_correlation.pkl', 'rb') as f:\n",
    "#     data = pickle.load(f)\n",
    "#     cv_models = data['models']\n",
    "#     scaler = data['scaler']\n",
    "#     final_features = data['selected_features']\n",
    "#     optimal_threshold = data['optimal_threshold']\n",
    "#     feature_types = data['feature_types']\n",
    "\n",
    "# ==============================================================================\n",
    "# HELPER FUNCTIONS (Training'dan copy qiling)\n",
    "# ==============================================================================\n",
    "\n",
    "def identify_feature_types(df, categorical_threshold=10):\n",
    "    \"\"\"Feature types aniqlash\"\"\"\n",
    "    numerical_features = []\n",
    "    categorical_features = []\n",
    "    \n",
    "    for col in df.columns:\n",
    "        if df[col].dtype in ['int64', 'float64']:\n",
    "            n_unique = df[col].nunique()\n",
    "            if n_unique > categorical_threshold:\n",
    "                numerical_features.append(col)\n",
    "            else:\n",
    "                categorical_features.append(col)\n",
    "        else:\n",
    "            categorical_features.append(col)\n",
    "    \n",
    "    return {\n",
    "        'numerical': numerical_features,\n",
    "        'categorical': categorical_features\n",
    "    }\n",
    "\n",
    "def engineer_features_minimal(df, feature_types=None):\n",
    "    \"\"\"Feature engineering - TRAINING BILAN BIR XIL\"\"\"\n",
    "    df_enh = df.copy()\n",
    "    \n",
    "    if not isinstance(df_enh, pd.DataFrame):\n",
    "        df_enh = pd.DataFrame(df_enh, columns=[f'feature_{i}' for i in range(df_enh.shape[1])])\n",
    "    \n",
    "    if feature_types is None:\n",
    "        feature_types = identify_feature_types(df_enh, categorical_threshold=10)\n",
    "    \n",
    "    numerical_cols = feature_types['numerical']\n",
    "    categorical_cols = feature_types['categorical']\n",
    "    \n",
    "    new_features = []\n",
    "    \n",
    "    # 1. Log transform (top 3)\n",
    "    for col in numerical_cols[:3]:\n",
    "        if col in df_enh.columns and (df_enh[col] > 0).all():\n",
    "            df_enh[f'{col}_log'] = np.log1p(df_enh[col])\n",
    "            new_features.append(f'{col}_log')\n",
    "    \n",
    "    # 2. Interactions (2x2)\n",
    "    if len(numerical_cols) >= 2:\n",
    "        for i in range(min(2, len(numerical_cols))):\n",
    "            for j in range(i+1, min(3, len(numerical_cols))):\n",
    "                col1, col2 = numerical_cols[i], numerical_cols[j]\n",
    "                if col1 in df_enh.columns and col2 in df_enh.columns:\n",
    "                    df_enh[f'{col1}_x_{col2}'] = df_enh[col1] * df_enh[col2]\n",
    "                    new_features.append(f'{col1}_x_{col2}')\n",
    "    \n",
    "    # 3. Ratios (top 2)\n",
    "    if len(numerical_cols) >= 2:\n",
    "        for i in range(min(2, len(numerical_cols))):\n",
    "            for j in range(i+1, min(3, len(numerical_cols))):\n",
    "                col1, col2 = numerical_cols[i], numerical_cols[j]\n",
    "                if col1 in df_enh.columns and col2 in df_enh.columns:\n",
    "                    if (df_enh[col2] != 0).all():\n",
    "                        df_enh[f'{col1}_div_{col2}'] = df_enh[col1] / (df_enh[col2] + 1e-5)\n",
    "                        new_features.append(f'{col1}_div_{col2}')\n",
    "    \n",
    "    # 4. Frequency encoding (categorical)\n",
    "    for cat_col in categorical_cols:\n",
    "        if cat_col in df_enh.columns:\n",
    "            freq = df_enh[cat_col].value_counts(normalize=True)\n",
    "            df_enh[f'{cat_col}_freq'] = df_enh[cat_col].map(freq).fillna(0)\n",
    "            new_features.append(f'{cat_col}_freq')\n",
    "    \n",
    "    # 5. Row statistics\n",
    "    if len(numerical_cols) >= 3:\n",
    "        selected = [col for col in numerical_cols[:min(5, len(numerical_cols))] if col in df_enh.columns]\n",
    "        if len(selected) >= 3:\n",
    "            df_enh['row_mean'] = df_enh[selected].mean(axis=1)\n",
    "            df_enh['row_std'] = df_enh[selected].std(axis=1)\n",
    "            df_enh['row_max'] = df_enh[selected].max(axis=1)\n",
    "            df_enh['row_min'] = df_enh[selected].min(axis=1)\n",
    "            new_features.extend(['row_mean', 'row_std', 'row_max', 'row_min'])\n",
    "    \n",
    "    # Fill missing\n",
    "    df_enh = df_enh.fillna(df_enh.median(numeric_only=True))\n",
    "    \n",
    "    return df_enh, new_features, feature_types\n",
    "\n",
    "def predict_with_cv_ensemble(models, X):\n",
    "    \"\"\"CV ensemble prediction\"\"\"\n",
    "    predictions = []\n",
    "    for model in models:\n",
    "        pred = model.predict_proba(X)[:, 1]\n",
    "        predictions.append(pred)\n",
    "    return np.mean(predictions, axis=0)\n",
    "\n",
    "# ==============================================================================\n",
    "# MAIN PREDICTION FUNCTION\n",
    "# ==============================================================================\n",
    "\n",
    "def predict_real_data(\n",
    "    real_data_path,\n",
    "    cv_models,\n",
    "    scaler,\n",
    "    final_features,\n",
    "    optimal_threshold,\n",
    "    feature_types,\n",
    "    extra_column_name=None,\n",
    "    output_path=\"real_predictions.csv\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Real data'ni predict qilish\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    real_data_path : str\n",
    "        Real data file path (CSV)\n",
    "    cv_models : list\n",
    "        Trained CV models\n",
    "    scaler : StandardScaler\n",
    "        Fitted scaler\n",
    "    final_features : list\n",
    "        Final selected feature names\n",
    "    optimal_threshold : float\n",
    "        Optimal threshold for classification\n",
    "    feature_types : dict\n",
    "        Feature types {'numerical': [...], 'categorical': [...]}\n",
    "    extra_column_name : str, optional\n",
    "        Ortiqcha column nomi (masalan: 'customer_id', 'id')\n",
    "    output_path : str\n",
    "        Output file path\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    DataFrame with predictions\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"STARTING PREDICTION PIPELINE\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # STEP 1: Load real data\n",
    "    print(\"\\nğŸ“‚ Loading real data...\")\n",
    "    real_df = pd.read_csv(real_data_path)\n",
    "    print(f\"âœ… Loaded: {real_df.shape}\")\n",
    "    print(f\"   Columns: {list(real_df.columns)}\")\n",
    "    \n",
    "    # STEP 2: Remove extra column if specified\n",
    "    if extra_column_name and extra_column_name in real_df.columns:\n",
    "        print(f\"\\nğŸ”§ Removing extra column: '{extra_column_name}'\")\n",
    "        extra_data = real_df[extra_column_name].copy()\n",
    "        real_df_clean = real_df.drop(columns=[extra_column_name])\n",
    "    else:\n",
    "        print(f\"\\nğŸ”§ No extra column to remove\")\n",
    "        extra_data = None\n",
    "        real_df_clean = real_df.copy()\n",
    "    \n",
    "    print(f\"âœ… Clean data: {real_df_clean.shape}\")\n",
    "    \n",
    "    # STEP 3: Feature engineering\n",
    "    print(f\"\\nğŸ”¨ Feature engineering...\")\n",
    "    real_df_eng, _, _ = engineer_features_minimal(real_df_clean, feature_types)\n",
    "    print(f\"âœ… Engineered: {real_df_eng.shape[1]} features\")\n",
    "    \n",
    "    # STEP 4: Check missing features\n",
    "    print(f\"\\nğŸ¯ Selecting required features...\")\n",
    "    missing = [f for f in final_features if f not in real_df_eng.columns]\n",
    "    \n",
    "    if missing:\n",
    "        print(f\"âš ï¸  {len(missing)} missing features (filling with 0):\")\n",
    "        for feat in missing[:5]:\n",
    "            print(f\"     - {feat}\")\n",
    "        if len(missing) > 5:\n",
    "            print(f\"     ... and {len(missing)-5} more\")\n",
    "        \n",
    "        for feat in missing:\n",
    "            real_df_eng[feat] = 0\n",
    "    \n",
    "    # Select features\n",
    "    real_X = real_df_eng[final_features]\n",
    "    print(f\"âœ… Selected: {real_X.shape[1]} features\")\n",
    "    \n",
    "    # STEP 5: Scaling\n",
    "    print(f\"\\nğŸ“ Scaling features...\")\n",
    "    real_X_scaled = pd.DataFrame(\n",
    "        scaler.transform(real_X),\n",
    "        columns=final_features,\n",
    "        index=real_X.index\n",
    "    )\n",
    "    print(f\"âœ… Scaled!\")\n",
    "    \n",
    "    # STEP 6: Prediction\n",
    "    print(f\"\\nğŸ”® Making predictions...\")\n",
    "    real_proba = predict_with_cv_ensemble(cv_models, real_X_scaled)\n",
    "    real_pred = (real_proba >= optimal_threshold).astype(int)\n",
    "    \n",
    "    print(f\"âœ… Predictions done!\")\n",
    "    print(f\"   Total: {len(real_pred)}\")\n",
    "    print(f\"   Class 0 (Approve): {(real_pred==0).sum()} ({(real_pred==0).sum()/len(real_pred)*100:.1f}%)\")\n",
    "    print(f\"   Class 1 (Reject):  {(real_pred==1).sum()} ({(real_pred==1).sum()/len(real_pred)*100:.1f}%)\")\n",
    "    \n",
    "    # STEP 7: Create results\n",
    "    print(f\"\\nğŸ“Š Creating results...\")\n",
    "    result = real_df.copy()\n",
    "    result[\"probability_score\"] = real_proba\n",
    "    result[\"prediction\"] = real_pred\n",
    "    result[\"risk_level\"] = pd.cut(\n",
    "        real_proba,\n",
    "        bins=[0, 0.3, 0.5, 0.7, 1.0],\n",
    "        labels=['Low', 'Medium', 'High', 'Very High']\n",
    "    )\n",
    "    \n",
    "    # Save\n",
    "    result.to_csv(output_path, index=False)\n",
    "    print(f\"âœ… Saved to: {output_path}\")\n",
    "    \n",
    "    # STEP 8: Summary\n",
    "    print(f\"\\n\" + \"=\"*70)\n",
    "    print(\"PREDICTION SUMMARY\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\nğŸ¯ Threshold: {optimal_threshold:.3f}\")\n",
    "    print(f\"\\nğŸ“Š Predictions:\")\n",
    "    print(result['prediction'].value_counts().to_string())\n",
    "    print(f\"\\nğŸ“Š Risk Levels:\")\n",
    "    print(result['risk_level'].value_counts().to_string())\n",
    "    print(f\"\\nğŸ“Š Score Statistics:\")\n",
    "    print(f\"   Min:    {real_proba.min():.4f}\")\n",
    "    print(f\"   Max:    {real_proba.max():.4f}\")\n",
    "    print(f\"   Mean:   {real_proba.mean():.4f}\")\n",
    "    print(f\"   Median: {np.median(real_proba):.4f}\")\n",
    "    \n",
    "    print(f\"\\nâœ… COMPLETED!\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    return result\n",
    "\n",
    "# ==============================================================================\n",
    "# QANDAY ISHLATISH\n",
    "# ==============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    print(\"\"\"\n",
    "    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "    â•‘         REAL DATA PREDICTION - USAGE EXAMPLE               â•‘\n",
    "    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "    \n",
    "    VARIANT 1: Agar model hali xotirada bo'lsa (training'dan keyim)\n",
    "    ================================================================\n",
    "    \n",
    "    result = predict_real_data(\n",
    "        real_data_path=\"real_data.csv\",\n",
    "        cv_models=cv_models,              # Training'dan\n",
    "        scaler=scaler,                     # Training'dan\n",
    "        final_features=final_features,     # Training'dan\n",
    "        optimal_threshold=optimal_threshold, # Training'dan\n",
    "        feature_types=feature_types,       # Training'dan\n",
    "        extra_column_name=\"customer_id\",   # Agar ortiqcha column bo'lsa\n",
    "        output_path=\"predictions.csv\"\n",
    "    )\n",
    "    \n",
    "    \n",
    "    VARIANT 2: Agar model pickle'da saqlangan bo'lsa\n",
    "    =================================================\n",
    "    \n",
    "    import pickle\n",
    "    \n",
    "    # Load model\n",
    "    with open('best_catboost_with_correlation.pkl', 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    \n",
    "    # Predict\n",
    "    result = predict_real_data(\n",
    "        real_data_path=\"real_data.csv\",\n",
    "        cv_models=data['models'],\n",
    "        scaler=data['scaler'],\n",
    "        final_features=data['selected_features'],\n",
    "        optimal_threshold=data['optimal_threshold'],\n",
    "        feature_types=data['feature_types'],\n",
    "        extra_column_name=None,  # Agar yo'q bo'lsa None\n",
    "        output_path=\"predictions.csv\"\n",
    "    )\n",
    "    \n",
    "    # Results\n",
    "    print(result.head(10))\n",
    "    \"\"\")\n",
    "    \n",
    "    # EXAMPLE: Uncomment quyidagi qatorlarni ishlatish uchun\n",
    "    # =========================================================\n",
    "    \n",
    "    # # Agar model xotirada bo'lsa:\n",
    "    result = predict_real_data(\n",
    "         real_data_path=\"../test/merged_x.csv\",\n",
    "         cv_models=cv_models,\n",
    "         scaler=scaler,\n",
    "         final_features=final_features,\n",
    "         optimal_threshold=optimal_threshold,\n",
    "         feature_types=feature_types,\n",
    "         extra_column_name=\"id\",  # yoki None\n",
    "         output_path=\"predictions.csv\"\n",
    "    )\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "da657191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… result.csv fayl yaratildi!\n",
      "   customer_id  probability_score  prediction\n",
      "0       100000           0.169370           0\n",
      "1       100001           0.199518           0\n",
      "2       100002           0.539271           1\n",
      "3       100003           0.410407           0\n",
      "4       100004           0.610621           1\n"
     ]
    }
   ],
   "source": [
    "# A.csv faylni oâ€˜qiymiz\n",
    "df = pd.read_csv(\"predictions.csv\")\n",
    "\n",
    "# Faqat kerakli ustunlar\n",
    "columns_to_keep = [\"customer_id\", \"probability_score\", \"prediction\"] \n",
    "\n",
    "# Yangi DataFrame\n",
    "new_df = df[columns_to_keep]\n",
    "\n",
    "# Yangi CSV ga saqlash\n",
    "new_df.to_csv(\"result.csv\", index=False)\n",
    "\n",
    "print(\"âœ… result.csv fayl yaratildi!\")\n",
    "print(new_df.head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
